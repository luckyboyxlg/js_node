<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<title>十一、selenium - js node</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="generator" content="mkdocs-1.6.1, mkdocs-gitbook-1.0.7">

<link rel="shortcut icon" href="./images/favicon.ico" type="image/x-icon">
<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta rel="next" href="" />
<link href="./css/style.min.css" rel="stylesheet"> 
</head>

<body>
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input type="text" placeholder="Type to search" />
</div> <!-- end of book-search-input -->

<nav role="navigation">
<ul class="summary">
<li>
<a href="index.html" target="_blank" class="custom-link">js node</a>
</li>
<li class="divider"></li>
<li class="chapter" data-path="index.html">
<a href="index.html">Welcome to MkDocs</a>
<li class="chapter" data-path="01%E3%80%81html.html">
<a href="01%E3%80%81html.html">一、HTML</a>
<li class="chapter" data-path="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">
<a href="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">二、CSS层叠样式表</a>
<li class="chapter" data-path="03%E3%80%81%E6%AD%A3%E5%88%99.html">
<a href="03%E3%80%81%E6%AD%A3%E5%88%99.html">三、正则</a>
<li class="chapter" data-path="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">
<a href="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">四、beautifulsoup</a>
<li class="chapter" data-path="05%E3%80%81xpath.html">
<a href="05%E3%80%81xpath.html">五、xpath</a>
<li class="chapter" data-path="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">
<a href="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">六、爬虫入门</a>
<li class="chapter" data-path="07%E3%80%81urllib%E4%B8%8Erequests.html">
<a href="07%E3%80%81urllib%E4%B8%8Erequests.html">七、urllib与requests</a>
<li class="chapter" data-path="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">
<a href="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">八、多进程</a>
<li class="chapter" data-path="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">
<a href="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">九、线程</a>
<li class="chapter" data-path="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">
<a href="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">十、携程</a>
<li class="chapter active" data-path="11%E3%80%81selenium.html">
<a href="11%E3%80%81selenium.html">十一、selenium</a>
<li class="chapter" data-path="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">
<a href="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">十二、MySQL数据库</a>
<li class="chapter" data-path="13%E3%80%81Mongodb.html">
<a href="13%E3%80%81Mongodb.html">十三、Mongodb</a>
<li class="chapter" data-path="14%E3%80%81redis.html">
<a href="14%E3%80%81redis.html">十四、Redis数据库</a>
<li class="chapter" data-path="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">
<a href="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">十五、面向对象</a>
<li class="chapter" data-path="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">
<a href="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">十六、Scrapy框架初认识</a>
<li class="chapter" data-path="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">
<a href="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">十六、Scrapy深入使用-存储</a>
<li class="chapter" data-path="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">
<a href="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">十六、scrapy模拟登陆&amp;分页</a>
<li class="chapter" data-path="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">
<a href="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">十六、Scrapy中间件</a>
<li class="chapter" data-path="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">
<a href="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">十六、scrapy的crawlspider爬虫</a>
<li class="chapter" data-path="16-Scrapy06-scrapy_redis.html">
<a href="16-Scrapy06-scrapy_redis.html">十六、scrapy_redis</a>
<li class="chapter" data-path="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">
<a href="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">十七、js数据类型</a>
<li class="chapter" data-path="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">
<a href="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">十八、js运算符与流程控制</a>
<li class="chapter" data-path="19%E3%80%81js%E6%95%B0%E7%BB%84.html">
<a href="19%E3%80%81js%E6%95%B0%E7%BB%84.html">十九、js数组</a>
<li class="chapter" data-path="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">
<a href="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">二十、js字符串</a>
<li class="chapter" data-path="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">
<a href="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">二十一、js对象与时间</a>
<li class="chapter" data-path="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">
<a href="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">二十二、js函数</a>
<li class="chapter" data-path="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">
<a href="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">二十三、Javascript进阶</a>
<li class="chapter" data-path="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">
<a href="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">二十四、浏览器对象模型BOM</a>
<li class="chapter" data-path="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">
<a href="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">二十五、DOM操作</a>
<li class="chapter" data-path="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">
<a href="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">二十六、jQuery</a>
<li class="chapter" data-path="27%E3%80%81%E9%80%86%E5%90%9101.html">
<a href="27%E3%80%81%E9%80%86%E5%90%9101.html">二十七、高级逆向01</a>
<li class="chapter" data-path="28%E3%80%81%E9%80%86%E5%90%9102.html">
<a href="28%E3%80%81%E9%80%86%E5%90%9102.html">二十八、高级逆向02</a>
<li class="chapter" data-path="29%E3%80%81%E9%80%86%E5%90%9103.html">
<a href="29%E3%80%81%E9%80%86%E5%90%9103.html">二十九、高级逆向03</a>
<li class="chapter" data-path="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">
<a href="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">三十、网易易盾</a>
<li class="chapter" data-path="31%E3%80%81rpc.html">
<a href="31%E3%80%81rpc.html">三十一、RPC</a>
<li class="chapter" data-path="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">
<a href="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">三十二、TLS指纹和JA3指纹绕过</a>
<li class="chapter" data-path="33%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">
<a href="33%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">三十三、微信小程序逆向开发</a>
<li class="chapter" data-path="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">
<a href="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">三十四、高级逆向</a>
<li class="chapter" data-path="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">
<a href="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">逆向实战案例笔记</a>
<li class="header">Pyexecjs与npm配置</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/execjs%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8.html" class="">Execjs介绍以及安装和使用</a>
</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/%E4%B8%80%E6%AC%A1%E6%80%A7%E8%A7%A3%E5%86%B3%E6%8E%89npm%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98.html" class="">更换npm为国内镜像</a>
</li>

<li class="header">补环境</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/1%E3%80%81%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%8E%E4%BA%8B%E4%BB%B6.html" class="">补环境</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/2%E3%80%81Proxy%E4%BB%A3%E7%90%86%E5%99%A8.html" class="">Proxy代理</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/3%E3%80%81vm2%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83.html" class="">3、vm2运行环境</a>
</li>

<li class="divider"></li>



<li><a href="http://www.mkdocs.org">
Published with MkDocs
</a></li>

<li><a href="https://github.com/GitbookIO/theme-default">
Theme by GitBook
</a></li>
</ul>

</nav>

</div> <!-- end of book-summary -->

<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">

<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="." ></a>
</h1>

</div> <!-- end of book-header -->

<div class="page-wrapper" tabindex="-1" role="main">
<div class="page-inner">
<div id="book-search-results">
<div class="search-noresults">

<section class="normal markdown-section">



<h1 id="selenium">十一、selenium</h1>
<h2 id="_1">一、前期准备</h2>
<h3 id="1">1、概述</h3>
<p>selenium本身是一个自动化测试工具。它可以让python代码调用浏览器。并获取到浏览器中加载的各种资源。 我们可以利用selenium提供的各项功能。 帮助我们完成数据的抓取。</p>
<h3 id="2">2、学习目标</h3>
<ol>
<li>掌握 selenium发送请求，加载网页的方法</li>
<li>掌握 selenium简单的元素定位的方法</li>
<li>掌握 selenium的基础属性和方法</li>
<li>掌握 selenium退出的方法</li>
</ol>
<h3 id="3">3、安装</h3>
<p>安装：pip install selenium</p>
<p>它与其他库不同的地方是他要启动你电脑上的浏览器, 这就需要一个驱动程序来辅助. </p>
<p>这里推荐用chrome浏览器</p>
<p>chrome驱动地址</p>
<p>http://chromedriver.storage.googleapis.com/index.html</p>
<p>https://registry.npmmirror.com/binary.html?path=chromedriver/</p>
<p>https://googlechromelabs.github.io/chrome-for-testing/#stable</p>
<p><img alt="image-20210125174618013" src="imgs/11%E3%80%81selenium.assets/image-20210125174618013.png" /></p>
<p><img alt="image-20210125174658971" src="imgs/11%E3%80%81selenium.assets/image-20210125174658971.png" /></p>
<p>根据你电脑的不同自行选择吧.  win64选win32即可.</p>
<p>然后关键的来了. 把你下载的浏览器驱动放在python解释器所在的文件夹</p>
<p>Windwos:  py -0p     查看Python路径</p>
<p>Mac: open + 路径</p>
<p>例如：open /usr/local/bin/</p>
<p><img alt="image-20210125175328245" src="imgs/11%E3%80%81selenium.assets/image-20210125175328245.png" /></p>
<p>前期准备工作完毕.  上代码看看 感受一下selenium</p>
<pre><code class="language-python">from selenium.webdriver import Chrome  # 导入谷歌浏览器的类


# 创建浏览器对象
web = Chrome()  # 如果你的浏览器驱动放在了解释器文件夹

web.get(&quot;http://www.baidu.com&quot;)  # 输入网址
print(web.title)  # 打印title
</code></pre>
<p>运行一下你会发现神奇的事情发生了. 浏览器自动打开了. 并且输入了网址. 也能拿到网页上的title标题. </p>
<p><img alt="image-20210125175906255" src="imgs/11%E3%80%81selenium.assets/image-20210125175906255.png" /></p>
<h2 id="selenium_1">二、selenium的基本使用</h2>
<h3 id="1_1">1、加载网页：</h3>
<p>selenium通过控制浏览器，所以对应的获取的数据都是elements中的内容</p>
<pre><code class="language-python">from selenium import webdriver
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()
# 访问百度
driver.get(&quot;http://www.baidu.com/&quot;)
# 截图
driver.save_screenshot(&quot;baidu.png&quot;)
</code></pre>
<h3 id="2_1">2、定位和操作：</h3>
<pre><code class="language-python"># 搜索关键字 杜卡迪
driver.find_element(By.ID, &quot;kw&quot;).send_keys(&quot;杜卡迪&quot;)
# 点击id为su的搜索按钮
driver.find_element(By.ID, &quot;su&quot;).click()
</code></pre>
<h3 id="3_1">3、查看请求信息：</h3>
<pre><code class="language-python">driver.page_source   # 获取页面内容
driver.get_cookies()
driver.current_url
</code></pre>
<h3 id="4">4、退出</h3>
<pre><code class="language-python">driver.close()  # 退出当前页面
driver.quit()   # 退出浏览器
</code></pre>
<h3 id="_2">小结</h3>
<ol>
<li>selenium的导包:<code>from selenium import webdriver</code></li>
<li>selenium创建driver对象:<code>webdriver.Chrome()</code></li>
<li>selenium请求数据:<code>driver.get("http://www.baidu.com/")</code></li>
<li>selenium查看数据: <code>driver.page_source</code></li>
<li>关闭浏览器: <code>driver.quit()</code></li>
<li>根据id定位元素: <code>driver.find_element_by_id("kw")/driver.find_element(By.ID, "kw")</code></li>
<li>操作点击事件: <code>click()</code></li>
<li>给输入框赋值:<code>send_keys()</code></li>
</ol>
<h2 id="_3">三、元素定位的方法</h2>
<h3 id="_4">学习目标</h3>
<ol>
<li>掌握 selenium定位元素的方法</li>
<li>掌握 selenium从元素中获取文本和属性的方法</li>
</ol>
<blockquote>
<p>通过selenium的基本使用可以简单定位元素和获取对应的数据,接下来我们再来学习下 定位元素的其他方法</p>
</blockquote>
<h3 id="1selenium">1、selenium的定位操作</h3>
<ol>
<li>
<p>元素定位的两种写法：</p>
</li>
<li>
<p>直接调用型</p>
<p><code>python
  el = driver.find_element_by_xxx(value)
  # xxx是定位方式，后面我们会讲，value为该方式对应的值</code></p>
</li>
<li>
<p>使用By类型(需要导入By)  建议使用这种方式</p>
<p><code>python
  # 直接掉用的方式会在底层翻译成这种方式
 from selenium.webdriver.common.by import By
 driver.find_element(By.xxx,value)</code></p>
</li>
<li>
<p>元素定位的两种方式:</p>
</li>
<li>
<p>精确定位一个元素,返回结果为一个element对象,定位不到则报错</p>
<p><code>python
 driver.find_element(By.xx, value)  # 建议使用
 driver.find_element_by_xxx(value)</code></p>
</li>
<li>
<p>定位一组元素,返回结果为element对象列表,定位不到返回空列表</p>
<p><code>python
 driver.find_elements(By.xx, value)  # 建议使用
 driver.find_elements_by_xxx(value)</code></p>
</li>
<li>
<p>元素定位的八种方法:</p>
</li>
</ol>
<p>以下方法在element之后添加s就变成能够获取一组元素的方法</p>
<ul>
<li>
<p>By.ID  使用id值定位</p>
<p><code>python
 el = driver.find_element(By.ID, '')
 el = driver.find_element_by_id()</code></p>
</li>
<li>
<p>By.XPATH 使用xpath定位</p>
<p><code>python
 el = driver.find_element(By.XPATH, '')
 el = driver.find_element_by_xpath()</code></p>
</li>
<li>
<p>By.TAG_NAME. 使用标签名定位</p>
<p><code>python
 el = driver.find_element(By.TAG_NAME, '')
 el = driver.find_element_by_tag_name()</code></p>
</li>
<li>
<p>By.LINK_TEXT使用超链接文本定位</p>
<p><code>python
 el = driver.find_element(By.LINK_TEXT, '')
 el = driver.find_element_by_link_text()</code></p>
</li>
<li>
<p>By.PARTIAL_LINK_TEXT  使用部分超链接文本定位</p>
<p><code>python
 el = driver.find_element(By.PARTIAL_LINK_TEXT  , '')
 el = driver.find_element_by_partial_link_text()</code></p>
</li>
<li>
<p>By.NAME   使用name属性值定位</p>
<p><code>python
 el = driver.find_element(By.NAME, '')
 el = driver.find_element_by_name()</code></p>
</li>
<li>
<p>By.CLASS_NAME     使用class属性值定位</p>
<p><code>python
 el = driver.find_element(By.CLASS_NAME, '')   
 el = driver.find_element_by_class_name()</code></p>
</li>
<li>
<p>By.CSS_SELECTOR   使用css选择器定位</p>
<p><code>python
 el = driver.find_element(By.CSS_SELECTOR, '')  
 el = driver.find_element_by_css_selector()</code></p>
</li>
</ul>
<p><strong>注意：</strong></p>
<ul>
<li>建议使用find_element/find_elements</li>
<li>
<p><code>find_element</code>和<code>find_elements</code>的区别 </p>
</li>
<li>
<p><code>by_link_text</code>和<code>by_partial_link_text</code>的区别：
  全部文本和包含某个文本</p>
</li>
<li>
<p>使用： 以豆瓣为例</p>
</li>
</ul>
<p>```python
  import time
  from selenium import webdriver
  from selenium.webdriver.common.by import By</p>
<p>driver = webdriver.Chrome()
  driver.implicitly_wait(10)  # 等待节点加载完成
  driver.get("https://www.douban.com/search?q=%E6%9D%B0%E6%A3%AE%E6%96%AF%E5%9D%A6%E6%A3%AE")
  time.sleep(2)
  # 使用id的方式获取右上角的搜索框
  # ret1 = driver.find_element(By.ID, 'inp-query')
  # ret1 = driver.find_element(By.ID, 'inp-query').send_keys("杰森斯坦森")
  # ret1 = driver.find_element_by_id("inp-query")
  # print(ret1)</p>
<p># 输出为：<selenium.webdriver.remote.webelement.WebElement (session="ea6f94544ac3a56585b2638d352e97f3", element="0.5335773935305805-1")></p>
<p># 搜索输入框  使用find_elements进行获取
  # ret2 = driver.find_elements(By.ID, "inp-query")
  # ret2 = driver.find_elements_by_id("inp-query")
  # print(ret2)
  #输出为：[<selenium.webdriver.remote.webelement.WebElement (session="ea6f94544ac3a56585b2638d352e97f3", element="0.5335773935305805-1")>]</p>
<p># 搜索按钮  使用xpath进行获取
  # ret3 = driver.find_elements(By.XPATH, '//<em>[@id="inp-query"]')
  # ret3 = driver.find_elements_by_xpath("//</em>[@id="inp-query"]")
  # print(len(ret3))
  # print(ret3)</p>
<p># 匹配图片标签
  ret4 = driver.find_elements(By.TAG_NAME, 'img')
  for url in ret4:
      print(url.get_attribute('src'))</p>
<p>#ret4 = driver.find_elements_by_tag_name("img")
  print(len(ret4))</p>
<p>ret5 = driver.find_elements(By.LINK_TEXT, "浏览发现")
  # ret5 = driver.find_elements_by_link_text("浏览发现")
  print(len(ret5))
  print(ret5)</p>
<p>ret6 = driver.find_elements(By.PARTIAL_LINK_TEXT, "浏览发现")
  # ret6 = driver.find_elements_by_partial_link_text("浏览发现")
  print(len(ret6))
  # 使用class名称查找
  ret7 = driver.find_elements(By.CLASS_NAME, 'nbg')
  print(ret7)
  driver.close()
  ```</p>
<p><strong>注意：</strong></p>
<p>find_element与find_elements区别</p>
<ol>
<li>只查找一个元素的时候:可以使用find_element(),find_elements()
     find_element()会返回一个WebElement节点对象,但是没找到会报错,而find_elements()不会,之后返回一个空列表</li>
<li>查找多个元素的时候:只能用find_elements(),返回一个列表,列表里的元素全是WebElement节点对象</li>
<li>找到都是节点(标签)</li>
<li>如果想要获取相关内容(只对find_element()有效,列表对象没有这个属性)  使用  .text</li>
<li>如果想要获取相关属性的值(如href对应的链接等,只对find_element()有效,列表对象没有这个属性):使用   .get_attribute("href")                                        </li>
</ol>
<h3 id="2_2">2、元素的操作</h3>
<blockquote>
<p>find_element_by_xxx方法仅仅能够获取元素对象，接下来就可以对元素执行以下操作 从定位到的元素中提取数据的方法</p>
</blockquote>
<ol>
<li>从定位到的元素中获取数据</li>
</ol>
<pre><code class="language-python">el.get_attribute(key)           # 获取key属性名对应的属性值
el.text                         # 获取开闭标签之间的文本内容
</code></pre>
<ol>
<li>对定位到的元素的操作</li>
</ol>
<pre><code class="language-python">el.click()                      # 对元素执行点击操作

el.submit()                     # 对元素执行提交操作

el.clear()                      # 清空可输入元素中的数据

el.send_keys(data)              # 向可输入元素输入数据
</code></pre>
<p>使用示例：</p>
<pre><code class="language-python">from selenium import webdriver
from selenium.webdriver.common.by import By

driver =webdriver.Chrome()

driver.get(&quot;https://www.douban.com/&quot;)
# 打印页面内容 （获取到以后可以进行后续的xpath,bs4 或者存储等）
print(driver.page_source)

ret4 = driver.find_elements(By.TAG_NAME, &quot;h1&quot;)
print(ret4[0].text)
#输出：豆瓣

ret5 = driver.find_elements(By.LINK_TEXT, &quot;下载豆瓣 App&quot;)
print(ret5[0].get_attribute(&quot;href&quot;))
#输出：https://www.douban.com/doubanapp/app?channel=nimingye

driver.close()
</code></pre>
<h3 id="_5">小结</h3>
<ol>
<li>根据xpath定位元素:<code>driver.find_elements(By.XPATH,"//*[@id='s']/h1/a")</code></li>
<li>根据class定位元素:<code>driver.find_elements(By.CLASS_NAME, "box")</code></li>
<li>根据link_text定位元素:<code>driver.find_elements(By.LINK_TEXT, "下载豆瓣 App")</code></li>
<li>根据tag_name定位元素:<code>driver.find_elements(By.TAG_NAME, "h1")</code></li>
<li>获取元素文本内容:<code>element.text</code></li>
<li>获取元素标签属性: <code>element.get_attribute("href")</code></li>
<li>向输入框输入数据: <code>element.send_keys(data)</code></li>
</ol>
<h2 id="selenium_2">四、selenium的其他操作</h2>
<h3 id="_6">学习目标</h3>
<ol>
<li>掌握 selenium处理cookie等方法</li>
<li>掌握 selenium中switch的使用</li>
<li>掌握selenium中无头浏览器的设置</li>
</ol>
<h3 id="1_2">1、无头浏览器</h3>
<p>我们已经基本了解了selenium的基本使用了. 但是呢, 不知各位有没有发现, 每次打开浏览器的时间都比较长. 这就比较耗时了. 我们写的是爬虫程序. 目的是数据. 并不是想看网页. 那能不能让浏览器在后台跑呢? 答案是可以的</p>
<pre><code class="language-python">from selenium.webdriver import Chrome
from selenium.webdriver.chrome.options import Options

opt = Options()
opt.add_argument(&quot;--headless&quot;)
opt.add_argument('--disable-gpu')
opt.add_argument(&quot;--window-size=4000,1600&quot;)  # 设置窗口大小

web = Chrome(options=opt)
</code></pre>
<h3 id="1selenium-cookie">1、selenium 处理cookie</h3>
<p>通过<code>driver.get_cookies()</code>能够获取所有的cookie</p>
<ul>
<li>获取cookie</li>
</ul>
<p><code>python
  dictCookies = driver.get_cookies()</code></p>
<ul>
<li>设置cookie</li>
</ul>
<p><code>python
  driver.add_cookie(dictCookies)</code></p>
<ul>
<li>删除cookue</li>
</ul>
<p><code>python
  #删除一条cookie
  driver.delete_cookie("CookieName")
  # 删除所有的cookie
  driver.delete_all_cookies()</code></p>
<h3 id="2_3">2、页面等待</h3>
<ul>
<li>
<p>为什么需要等待
  如果网站采用了动态html技术，那么页面上的部分元素出现时间便不能确定，这个时候就可以设置一个等待时间，强制等待指定时间，等待结束之后进行元素定位，如果还是无法定位到则报错 </p>
</li>
<li>
<p>页面等待的三种方法</p>
</li>
<li>
<p>强制等待</p>
<p><code>python
import time
time.sleep(n)      # 阻塞等待设定的秒数之后再继续往下执行</code></p>
</li>
<li>
<p>显式等待(自动化web测试使用，爬虫基本不用)</p>
<p>```python
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC</p>
<p>WebDriverWait(driver, 10,0.5).until( EC.presence_of_element_located((By.ID, "myDynamicElement"))</p>
<h1 id="10100510">显式等待指定某个条件，然后设置最长等待时间10，在10秒内每隔0.5秒使用指定条件去定位元素，如果定位到元素则直接结束等待，如果在10秒结束之后仍未定位到元素则报错</h1>
<p>```</p>
</li>
<li>
<p>隐式等待 隐式等待设置之后代码中的所有元素定位都会做隐式等待</p>
<p><code>python
driver.implicitly_wait(10)    # 在指定的n秒内每隔一段时间尝试定位元素，如果n秒结束还未被定位出来则报错</code></p>
</li>
</ul>
<p><strong>注意：</strong></p>
<p>Selenium显示等待和隐式等待的区别
1、selenium的显示等待
原理：显示等待，就是明确要等到某个元素的出现或者是某个元素的可点击等条件，等不到，就一直等，除非在规定的时间之内都没找到，就会跳出异常Exception</p>
<p>(简而言之，就是直到元素出现才去操作，如果超时则报异常)</p>
<p>2、selenium的隐式等待</p>
<p>原理：隐式等待，就是在创建driver时，为浏览器对象创建一个等待时间，这个方法是得不到某个元素就等待一段时间，直到拿到某个元素位置。
注意：在使用隐式等待的时候，实际上浏览器会在你自己设定的时间内部断的刷新页面去寻找我们需要的元素</p>
<h3 id="3switch">3、switch方法切换的操作</h3>
<h5 id="31">3.1 一个浏览器肯定会有很多窗口，所以我们肯定要有方法来实现窗口的切换。切换窗口的方法如下：</h5>
<pre><code class="language-python">也可以使用 window_handles 方法来获取每个窗口的操作对象。例如：

# 1. 获取当前所有的窗口
current_windows = driver.window_handles

# 2. 根据窗口索引进行切换
driver.switch_to.window(current_windows[1])

driver.switch_to.window(web.window_handles[-1])  # 跳转到最后一个窗口
driver.switch_to.window(current_windows[0])  # 回到第一个窗口
</code></pre>
<h5 id="32-iframehtmlseleniumframe">3.2 iframe是html中常用的一种技术，即一个页面中嵌套了另一个网页，selenium默认是访问不了frame中的内容的，对应的解决思路是</h5>
<pre><code class="language-python">driver.switch_to.frame(name/el/id)     传入的参数可以使iframe对应的id值，也可以是用元素定位之后的元素对象
</code></pre>
<p>动手：qq邮箱</p>
<p>在使用selenium登录qq邮箱的过程中，我们会发现，无法在邮箱的登录input标签中输入内容，通过观察源码可以发现，form表单在一个frame中，所以需要切换到frame中</p>
<h5 id="33">3.3 当你触发了某个事件之后，页面出现了弹窗提示，处理这个提示或者获取提示信息方法如下：</h5>
<pre><code class="language-python">alert = driver.switch_to_alert()
</code></pre>
<h5 id="4_1">4. 页面前进和后退</h5>
<pre><code class="language-python">driver.forward()     # 前进
driver.back()        # 后退
driver.refresh()         # 刷新
driver.close()       # 关闭当前窗口
</code></pre>
<h5 id="5">5、设置浏览器最大窗口</h5>
<pre><code>driver.maximize_window()  #最大化浏览器窗口
</code></pre>
<h3 id="4selenium">4、selenium的优缺点</h3>
<ul>
<li>优点</li>
<li>selenium能够执行页面上的js，对于js渲染的数据和模拟登陆处理起来非常容易</li>
<li>使用难度简单</li>
<li>爬取速度慢，爬取频率更像人的行为，天生能够应对一些反爬措施</li>
<li>缺点</li>
<li>由于selenium操作浏览器，因此会将发送所有的请求，因此占用网络带宽</li>
<li>由于操作浏览器，因此占用的内存非常大(相比较之前的爬虫)</li>
<li>速度慢，对于效率要求高的话不建议使用</li>
</ul>
<h3 id="_7">小结</h3>
<ol>
<li>获取cookie: <code>get_cookies()</code></li>
<li>删除cookie: <code>delete_all_cookies()</code></li>
<li>切换窗口:<code>switch_to.window()</code></li>
<li>切换iframe: <code>switch_to.frame()</code></li>
</ol>
<h3 id="_8">五、扩展</h3>
<p><a href="https://github.com/wkeeling/selenium-wire">Seleniumwire</a> 扩展了 <a href="https://so.csdn.net/so/search?q=Selenium&amp;spm=1001.2101.3001.7020">Selenium</a> 的 Python 绑定，让您可以访问浏览器发出的底层请求。 您编写代码的方式与使用 Selenium 的方式相同，但您可以获得额外的 API 来检查请求和响应并动态更改它们。</p>
<p><strong>1.环境要求</strong></p>
<ul>
<li>Python 3.6+</li>
<li>Selenium 3.4.0+</li>
<li>Chrome, Firefox and Remote Webdriver supported</li>
</ul>
<p><strong>2.安装</strong></p>
<p>pip install selenium-wire</p>
<p><strong>3.示例</strong></p>
<pre><code class="language-python">from seleniumwire import webdriver

driver = webdriver.Chrome()

driver.get('https://www.baidu.com')

for request in driver.requests:
    if request.response:
        print(
            request.url,
            request.response.status_code,
            request.response.headers['Content-Type'])
</code></pre>
<p>4.总结
该模块常用于需要登录网站的cookies和token等加密参数的获取，为后续采集程序提供通行凭证。</p>
<ol>
<li>selenium的配置</li>
</ol>
<p>https://blog.csdn.net/qq_35999017/article/details/123922952</p>
<p>https://blog.csdn.net/qq_27109535/article/details/125468643</p>


</section>
</div> <!-- end of search-noresults -->
<div class="search-results">
<div class="has-results">

<h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
<ul class="search-results-list"></ul>

</div> <!-- end of has-results -->
<div class="no-results">

<h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>

</div> <!-- end of no-results -->
</div> <!-- end of search-results -->
</div> <!-- end of book-search-results -->

</div> <!-- end of page-inner -->
</div> <!-- end of page-wrapper -->

</div> <!-- end of body-inner -->

</div> <!-- end of book-body -->
<script src="./js/main.js"></script>
<script src="search/main.js"></script>
<script src="./js/gitbook.min.js"></script>
<script src="./js/theme.min.js"></script>
</body>
</html>