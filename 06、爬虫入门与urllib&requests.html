<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<title>六、爬虫入门 - js node</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="generator" content="mkdocs-1.6.1, mkdocs-gitbook-1.0.7">

<link rel="shortcut icon" href="./images/favicon.ico" type="image/x-icon">
<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta rel="next" href="" />
<link href="./css/style.min.css" rel="stylesheet"> 
</head>

<body>
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input type="text" placeholder="Type to search" />
</div> <!-- end of book-search-input -->

<nav role="navigation">
<ul class="summary">
<li>
<a href="index.html" target="_blank" class="custom-link">js node</a>
</li>
<li class="divider"></li>
<li class="chapter" data-path="index.html">
<a href="index.html">Welcome to MkDocs</a>
<li class="chapter" data-path="01%E3%80%81html.html">
<a href="01%E3%80%81html.html">一、HTML</a>
<li class="chapter" data-path="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">
<a href="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">二、CSS层叠样式表</a>
<li class="chapter" data-path="03%E3%80%81%E6%AD%A3%E5%88%99.html">
<a href="03%E3%80%81%E6%AD%A3%E5%88%99.html">三、正则</a>
<li class="chapter" data-path="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">
<a href="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">四、beautifulsoup</a>
<li class="chapter" data-path="05%E3%80%81xpath.html">
<a href="05%E3%80%81xpath.html">五、xpath</a>
<li class="chapter active" data-path="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">
<a href="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">六、爬虫入门</a>
<li class="chapter" data-path="07%E3%80%81urllib%E4%B8%8Erequests.html">
<a href="07%E3%80%81urllib%E4%B8%8Erequests.html">七、urllib与requests</a>
<li class="chapter" data-path="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">
<a href="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">八、多进程</a>
<li class="chapter" data-path="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">
<a href="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">九、线程</a>
<li class="chapter" data-path="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">
<a href="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">十、携程</a>
<li class="chapter" data-path="11%E3%80%81selenium.html">
<a href="11%E3%80%81selenium.html">十一、selenium</a>
<li class="chapter" data-path="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">
<a href="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">十二、MySQL数据库</a>
<li class="chapter" data-path="13%E3%80%81Mongodb.html">
<a href="13%E3%80%81Mongodb.html">十三、Mongodb</a>
<li class="chapter" data-path="14%E3%80%81redis.html">
<a href="14%E3%80%81redis.html">十四、Redis数据库</a>
<li class="chapter" data-path="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">
<a href="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">十五、面向对象</a>
<li class="chapter" data-path="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">
<a href="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">十六、Scrapy框架初认识</a>
<li class="chapter" data-path="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">
<a href="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">十六、Scrapy深入使用-存储</a>
<li class="chapter" data-path="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">
<a href="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">十六、scrapy模拟登陆&amp;分页</a>
<li class="chapter" data-path="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">
<a href="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">十六、Scrapy中间件</a>
<li class="chapter" data-path="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">
<a href="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">十六、scrapy的crawlspider爬虫</a>
<li class="chapter" data-path="16-Scrapy06-scrapy_redis.html">
<a href="16-Scrapy06-scrapy_redis.html">十六、scrapy_redis</a>
<li class="chapter" data-path="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">
<a href="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">十七、js数据类型</a>
<li class="chapter" data-path="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">
<a href="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">十八、js运算符与流程控制</a>
<li class="chapter" data-path="19%E3%80%81js%E6%95%B0%E7%BB%84.html">
<a href="19%E3%80%81js%E6%95%B0%E7%BB%84.html">十九、js数组</a>
<li class="chapter" data-path="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">
<a href="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">二十、js字符串</a>
<li class="chapter" data-path="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">
<a href="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">二十一、js对象与时间</a>
<li class="chapter" data-path="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">
<a href="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">二十二、js函数</a>
<li class="chapter" data-path="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">
<a href="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">二十三、Javascript进阶</a>
<li class="chapter" data-path="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">
<a href="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">二十四、浏览器对象模型BOM</a>
<li class="chapter" data-path="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">
<a href="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">二十五、DOM操作</a>
<li class="chapter" data-path="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">
<a href="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">二十六、jQuery</a>
<li class="chapter" data-path="27%E3%80%81%E9%80%86%E5%90%9101.html">
<a href="27%E3%80%81%E9%80%86%E5%90%9101.html">二十七、高级逆向01</a>
<li class="chapter" data-path="28%E3%80%81%E9%80%86%E5%90%9102.html">
<a href="28%E3%80%81%E9%80%86%E5%90%9102.html">二十八、高级逆向02</a>
<li class="chapter" data-path="29%E3%80%81%E9%80%86%E5%90%9103.html">
<a href="29%E3%80%81%E9%80%86%E5%90%9103.html">二十九、高级逆向03</a>
<li class="chapter" data-path="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">
<a href="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">三十、网易易盾</a>
<li class="chapter" data-path="31%E3%80%81rpc.html">
<a href="31%E3%80%81rpc.html">三十一、RPC</a>
<li class="chapter" data-path="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">
<a href="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">三十二、TLS指纹和JA3指纹绕过</a>
<li class="chapter" data-path="33%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">
<a href="33%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">三十三、微信小程序逆向开发</a>
<li class="chapter" data-path="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">
<a href="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">三十四、高级逆向</a>
<li class="chapter" data-path="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">
<a href="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">逆向实战案例笔记</a>
<li class="header">Pyexecjs与npm配置</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/execjs%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8.html" class="">Execjs介绍以及安装和使用</a>
</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/%E4%B8%80%E6%AC%A1%E6%80%A7%E8%A7%A3%E5%86%B3%E6%8E%89npm%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98.html" class="">更换npm为国内镜像</a>
</li>

<li class="header">补环境</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/1%E3%80%81%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%8E%E4%BA%8B%E4%BB%B6.html" class="">补环境</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/2%E3%80%81Proxy%E4%BB%A3%E7%90%86%E5%99%A8.html" class="">Proxy代理</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/3%E3%80%81vm2%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83.html" class="">3、vm2运行环境</a>
</li>

<li class="divider"></li>



<li><a href="http://www.mkdocs.org">
Published with MkDocs
</a></li>

<li><a href="https://github.com/GitbookIO/theme-default">
Theme by GitBook
</a></li>
</ul>

</nav>

</div> <!-- end of book-summary -->

<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">

<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="." ></a>
</h1>

</div> <!-- end of book-header -->

<div class="page-wrapper" tabindex="-1" role="main">
<div class="page-inner">
<div id="book-search-results">
<div class="search-noresults">

<section class="normal markdown-section">



<h1 id="_1">六、爬虫入门</h1>
<h2 id="_2">一、爬虫概念</h2>
<h3 id="1">1、什么是爬虫？</h3>
<p>程序猿：写程序，然后去互联网上抓取数据的过程
互联网：网，有好多的a链接组成的，网的节点就是每一个a链接   url（统一资源定位符）</p>
<h3 id="2">2、哪些语言可以实现爬虫？</h3>
<ul>
<li>php，可以做，号称世界上最优美的语言，多进程、多线程支持的不好</li>
<li>java，也可以做爬虫，人家做的很好，最主要的竞争对手，代码臃肿，重构成本大</li>
<li>c、c++，是你能力的体现，不是良好的选择</li>
<li>python，世界上最美丽的语言，语法简单、代码优美，学习成本低，支持的模块多，非常强大的框架scrapy</li>
</ul>
<h3 id="3">3、爬虫分类</h3>
<ul>
<li>
<p>通用爬虫</p>
</li>
<li>
<p>实例</p>
<ul>
<li>百度、360、google、sougou等搜索引擎</li>
</ul>
<p>将网上的所有数据进行爬取，对数据进行排名，搜索之后会按排名展示数据</p>
</li>
<li>
<p>功能</p>
<ul>
<li>访问网页-&gt;抓取数据-&gt;数据存储-&gt;数据处理-&gt;提供检索服务</li>
</ul>
</li>
<li>
<p>抓取流程</p>
<ul>
<li>
<ol>
<li>给定一些起始的URL，放入待爬取队列</li>
</ol>
</li>
</ul>
<p>很多网页都有友情链接，如果爬虫漫无目的的爬取数据会爬到其他网站，不同的网站都会存在不同的外部链接，所以有可能会重复，从队列中获取可以避免重复网址的爬取</p>
<ul>
<li>
<ol>
<li>从队列中获取url对象，开始爬取数据</li>
</ol>
</li>
<li>
<ol>
<li>分析网页，获取网页内的所有url，入队，继续重复执行第二步</li>
</ol>
</li>
</ul>
</li>
<li>
<p>搜索引擎如何获取新网站链接</p>
<ul>
<li>
<ol>
<li>主动给搜索引擎提交url</li>
</ol>
</li>
<li>
<ol>
<li>在其他网站中设置友情链接</li>
</ol>
</li>
<li>
<ol>
<li>百度和DNS服务商合作，只要有域名，就会收录新网站</li>
</ol>
</li>
</ul>
</li>
<li>
<p>robots协议</p>
<ul>
<li>一个约定俗成的协议，添加robots.txt文件，来说明本网站哪些内容不可以被抓取，起不到限制作用</li>
<li>自己写的爬虫无需遵守</li>
</ul>
</li>
<li>
<p>网站排名(SEO)</p>
<p>一般公司会有SEO人员负责自己官网的网站排名</p>
<p>SEM</p>
<ul>
<li>
<ol>
<li>根据pagerank值进行排名（参考个网站流量、点击率等指标）</li>
</ol>
</li>
<li>
<ol>
<li>百度竞价排名，钱多就是爸爸</li>
</ol>
</li>
</ul>
</li>
<li>
<p>缺点</p>
<ul>
<li>
<ol>
<li>抓取的数据大多是无用的</li>
</ol>
</li>
<li>2.不能根据用户的需求来精准获取数据</li>
</ul>
</li>
<li>
<p>聚焦爬虫</p>
</li>
<li>
<p>功能</p>
<ul>
<li>根据需求，实现爬虫程序，抓取需要的数据</li>
</ul>
</li>
<li>
<p>原理</p>
<ul>
<li>1.网页都有自己唯一的url(统一资源定位符）</li>
<li>2.网页都是html组成</li>
<li>3.传输协议都是http\https</li>
</ul>
</li>
<li>
<p>设计思路</p>
<ul>
<li>
<p>1.确定要爬取的url</p>
</li>
<li>
<p>如何获取Url</p>
</li>
<li>
<p>2.模拟浏览器通过http协议访问url，获取服务器返回的html代码</p>
</li>
<li>
<p>如何访问</p>
</li>
<li>
<p>3.解析html字符串（根据一定规则提取需要的数据）</p>
</li>
<li>
<p>如何解析</p>
</li>
</ul>
</li>
<li>
<p>增量爬虫</p>
</li>
<li>
<p>数据量大的问题的一般解决思路</p>
<ul>
<li>
<ol>
<li>优化系统的能力，即执行效率提高</li>
</ol>
</li>
<li>
<p>并行、分布式、集群</p>
</li>
<li>
<ol>
<li>提高系统的工作效率，即降低运算成本</li>
</ol>
</li>
<li>
<p>优化算法、降低算法复杂度</p>
<ul>
<li>时间复杂度</li>
<li>空间复杂度</li>
</ul>
</li>
</ul>
</li>
<li>
<p>思考：如果数据库中有3亿条数据，如何解决？</p>
</li>
<li>
<p>增量爬虫：</p>
<ul>
<li>
<p>属于提高系统工作效率的一种解决思路</p>
</li>
<li>
<p>即：如果网站内容更新，再次爬取时不再爬取重复的信息</p>
</li>
<li>
<p>概念：网站内容的新增（包括数据的新增和页面的新增）称之为增量。爬取过程称为增量爬虫。</p>
</li>
<li>
<p>设计思路（去重）：</p>
</li>
<li>
<ol>
<li>在发送请求前判断此url是否爬取过</li>
</ol>
</li>
<li>
<ol>
<li>在解析内容后判断这部分内容是否爬取过</li>
</ol>
</li>
<li>
<ol>
<li>写入存储空间时判断内容是否存在</li>
</ol>
</li>
<li>eg. 使用hash值生成数据指纹来去重，高效，节省时间</li>
<li>eg. 使用Redis数据库的集合类型存储数据，具有无序不重复的特点</li>
</ul>
</li>
</ul>
<h3 id="4">4、深度优先和广度优先</h3>
<ul>
<li>
<p>深度优先：</p>
</li>
<li>
<p>指网络爬虫会从起始页开始，一个链接一个链接跟踪下去，处理完这条线路之后再转入下一个起始页，继续追踪链接</p>
<p>这里是深度优先，所以这里的爬取的顺序式：</p>
<p>A-B-D-E-I-C-F-G-H (递归实现)</p>
</li>
<li>
<p>基本思路都是深度优先，包括scrapy</p>
</li>
<li>
<p>广度优先：</p>
</li>
<li>
<p>也叫宽度优先，是指将新下载网页发现的链接直接插入到待抓取URL队列的末尾，也就是指网络爬虫会先抓取起始页中的所有网页，然后在选择其中的一个连接网页，继续抓取在此网页中链接的所有网页</p>
<p>这个图为例子，广度优先的爬取顺序为：</p>
<p>A-B-C-D-E-F-G-H-I (队列实现)</p>
</li>
</ul>
<h2 id="_3">二、整体内容</h2>
<h3 id="1python">1、python语法</h3>
<h3 id="2python">2、使用的python库</h3>
<ul>
<li>urllib.request</li>
<li>urllib.parse</li>
<li>requests</li>
</ul>
<h3 id="3_1">3、解析内容</h3>
<ul>
<li>正则表达式</li>
<li>xpath</li>
<li>
<p>推荐使用xpath</p>
</li>
<li>
<p>bs4</p>
</li>
<li>
<p>jsonpath</p>
</li>
</ul>
<h3 id="4html">4、采集动态html</h3>
<p>因为所有的网站都不止存在一个请求（js.csss等动态请求），如果仅仅对网站首页发送请求，会导致网站内容接受不全</p>
<p>解决方案：模拟浏览器进行访问</p>
<p>selenium+headless</p>
<h3 id="5scrapy">5、scrapy</h3>
<p>高性能异步网络爬虫框架</p>
<h3 id="6">6、分布式爬虫</h3>
<p>scrapy-redis组件</p>
<p>在scrapy基础上增加了一套组件，结合redis进行存储等功能</p>
<h3 id="7">7、反爬虫的一般手段</h3>
<p>爬虫项目最复杂的不是页面信息的提取，反而是爬虫与反爬虫、反反爬虫的博弈过程</p>
<ul>
<li>User-Agent</li>
</ul>
<p>浏览器的标志信息，会通过请求头传递给服务器，用以说明访问数据的浏览器信息</p>
<p>反爬虫：先检查是否有UA，或者UA是否合法</p>
<ul>
<li>
<p>代理IP</p>
</li>
<li>
<p>西次代理</p>
</li>
<li>
<p>验证码访问</p>
</li>
</ul>
<p>把验证码图片下载下来，然后手动输入</p>
<ul>
<li>
<p>打码平台</p>
</li>
<li>
<p>动态加载网页</p>
</li>
<li>
<p>数据加密</p>
</li>
<li>
<p>分析js代码</p>
</li>
<li>
<p>爬虫-反爬虫-反反爬虫</p>
</li>
</ul>
<h2 id="web">三、web请求全过程剖析(重点)</h2>
<h3 id="1_1">1、概述</h3>
<p>上一小节我们实现了一个网页的整体抓取工作. 那么本小节, 给各位好好剖析一下web请求的全部过程, 这样有助于后面我们遇到的各种各样的网站就有了入手的基本准则了. </p>
<p>那么到底我们浏览器在输入完网址到我们看到网页的整体内容, 这个过程中究竟发生了些什么?</p>
<p>这里我们以百度为例.  在访问百度的时候, 浏览器会把这一次请求发送到百度的服务器(百度的一台电脑), 由服务器接收到这个请求, 然后加载一些数据. 返回给浏览器, 再由浏览器进行显示. 听起来好像是个废话...但是这里蕴含着一个极为重要的东西在里面, 注意, 百度的服务器返回给浏览器的不直接是页面, 而是页面源代码(由html, css, js组成). 由浏览器把页面源代码进行执行, 然后把执行之后的结果展示给用户. 所以我们能看到在上一节的内容中,我们拿到的是百度的源代码(就是那堆看不懂的鬼东西). 具体过程如图.</p>
<p><img alt="image-20201215173513873" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20201215173513873.png" /></p>
<p>接下来就是一个比较重要的事情了. 所有的数据都在页面源代码里么? 非也~ 这里要介绍一个新的概念</p>
<p>那就是页面渲染数据的过程, 我们常见的页面渲染过程有两种, </p>
<ol>
<li>服务器渲染, 你需要的数据直接在页面源代码里能搜到</li>
</ol>
<p>这个最容易理解, 也是最简单的. 含义呢就是我们在请求到服务器的时候, 服务器直接把数据全部写入到html中, 我们浏览器就能直接拿到带有数据的html内容. 比如, </p>
<p><img alt="image-20201215173905476" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20201215173905476.png" /></p>
<p>由于数据是直接写在html中的, 所以我们能看到的数据都在页面源代码中能找的到的. </p>
<p>这种网页一般都相对比较容易就能抓取到页面内容. </p>
<ol>
<li>前端JS渲染, 你需要的数据在页面源代码里搜不到</li>
</ol>
<p>这种就稍显麻烦了. 这种机制一般是第一次请求服务器返回一堆HTML框架结构. 然后再次请求到真正保存数据的服务器, 由这个服务器返回数据, 最后在浏览器上对数据进行加载. 就像这样:</p>
<p><img alt="image-20201215174726729" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20201215174726729.png" /></p>
<p>这样做的好处是服务器那边能缓解压力. 而且分工明确. 比较容易维护. 典型的有这么一个网页</p>
<p><img alt="image-20201215175207478" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20201215175207478.png" /></p>
<p>那数据是何时加载进来的呢?  其实就是在我们进行页面向下滚动的时候, jd就在偷偷的加载数据了, 此时想要看到这个页面的加载全过程, 我们就需要借助浏览器的调试工具了(F12)</p>
<p><img alt="image-20201215175536447" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20201215175536447.png" /></p>
<p><img alt="image-20201215175637599" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20201215175637599.png" /></p>
<p><img alt="image-20201215175848471" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20201215175848471.png" /></p>
<p><img alt="image-20201215180141450" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20201215180141450.png" /></p>
<p>看到了吧, 页面上看到的内容其实是后加载进来的. </p>
<p>OK, 在这里我不是要跟各位讲jd有多牛B, 也不是说这两种方式有什么不同, 只是想告诉各位, 有些时候, 我们的数据不一定都是直接来自于页面源代码.  如果你在页面源代码中找不到你要的数据时, 那很可能数据是存放在另一个请求里. </p>
<pre><code> 1.你要的东西在页面源代码. 直接拿`源代码`提取数据即可
 2.你要的东西，不在页面源代码, 需要想办法找到真正的加载数据的那个请求. 然后提取数据
</code></pre>
<h3 id="2web">2、最简单的web应用程序</h3>
<p><img alt="image-20220901214542004-0287919" src="./imgs/./imgs/06、爬虫入门与urllib&amp;requests.assets/image-20220901214542004-0287919.png" /></p>
<h4 id="1http">【1】http协议特性</h4>
<p>HTTP协议是Hyper Text Transfer Protocol（超文本传输协议）的缩写,是用于万维网（WWW:World Wide Web ）服务器与本地浏览器之间传输超文本的传送协议。HTTP是一个属于应用层的面向对象的协议，由于其简捷、快速的方式，适用于分布式超媒体信息系统。它于1990年提出，经过几年的使用与发展，得到不断地完善和扩展。HTTP协议工作于客户端-服务端架构为上。浏览器作为HTTP客户端通过URL向HTTP服务端即WEB服务器发送所有请求。Web服务器根据接收到的请求后，向客户端发送响应信息。</p>
<p><img alt="image-20241030下午71036559-0286637" src="./imgs/./imgs/06、爬虫入门与urllib&amp;requests.assets/image-20241030下午71036559-0286637.png" /></p>
<ul>
<li>
<p>基于TCP/IP协议</p>
</li>
<li>
<p>基于请求－响应模式</p>
</li>
<li>
<p>无状态保存</p>
</li>
<li>
<p>短连接和长连接</p>
</li>
</ul>
<p>HTTP1.0默认使用的是短连接。浏览器和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。
  HTTP/1.1起，默认使用长连接。要使用长连接，客户端和服务器的HTTP首部的Connection都要设置为keep-alive，才能支持长连接。
  HTTP长连接，指的是复用TCP连接。多个HTTP请求可以复用同一个TCP连接，这就节省了TCP连接建立和断开的消耗。</p>
<h4 id="2http">【2】http请求协议与响应协议</h4>
<p>http协议包含由浏览器发送数据到服务器需要遵循的请求协议与服务器发送数据到浏览器需要遵循的请求协议。用于HTTP协议交互的信被为HTTP报文。请求端(客户端)的HTTP报文 做请求报文,响应端(服务器端)的 做响应报文。HTTP报文本身是由多行数据构成的字文本。</p>
<ul>
<li>一个完整的URL包括：协议、ip、端口、路径、参数  （lagou网）</li>
<li>请求方式: get与post请求</li>
</ul>
<p>请求协议格式：</p>
<p><img alt="image-20241030下午64419026" src="./imgs/./imgs/06、爬虫入门与urllib&amp;requests.assets/image-20241030下午64419026-0285059.png" /></p>
<p>响应协议格式：</p>
<p><img alt="image-20241030下午64434183" src="./imgs/./imgs/06、爬虫入门与urllib&amp;requests.assets/image-20241030下午64434183-0285075.png" /></p>
<p><img alt="http协议" src="./imgs/./imgs/06、爬虫入门与urllib&amp;requests.assets/http协议.png" /></p>
<h4 id="3getpost">【3】get请求和post请求</h4>
<p>HTTP 协议是用于在客户端（如浏览器）和服务器之间传输数据的协议。它定义了多种请求方法，其中最常用的两种是 <strong>GET</strong> 和 <strong>POST</strong> 请求。以下是对这两种请求的详细介绍：</p>
<p>GET 请求</p>
<ul>
<li><strong>定义</strong>：GET 请求用于从服务器获取数据。它是无副作用的，即不会对服务器上的资源产生改变。</li>
<li>特点：</li>
<li><strong>参数传递</strong>：请求参数通常附加在 URL 中，通过 <code>?</code> 和 <code>&amp;</code> 分隔。例如：<code>https://example.com/api?name=John&amp;age=30</code>。</li>
<li><strong>限制</strong>：URL 的长度有限制，这通常取决于浏览器和服务器，实现上大约为 2000 字符，因此不适合传递大量数据。</li>
<li><strong>安全性</strong>：因为参数在 URL 中明文显示，所以 GET 请求不适合传递敏感信息（如密码）。</li>
</ul>
<p>POST 请求</p>
<ul>
<li><strong>定义</strong>：POST 请求用于向服务器发送数据，通常用于创建或更新资源。</li>
<li>特点：</li>
<li><strong>参数传递</strong>：请求参数包含在请求体中，而不是 URL 中。这使得可以传递大量数据。</li>
<li><strong>灵活性</strong>：可以处理多种类型的数据，比如 JSON、XML、表单数据等。</li>
<li><strong>安全性</strong>：虽然 POST 请求比 GET 请求更安全（因为数据不暴露在 URL 中），但仍需通过 HTTPS 进行加密，以保护敏感信息。</li>
</ul>
<h4 id="4htmljson">【4】响应html和json数据类型</h4>
<h4 id="5">【5】流程</h4>
<p><img alt="image-20241030下午75531809-0289332" src="./imgs/./imgs/06、爬虫入门与urllib&amp;requests.assets/image-20241030下午75531809-0289332.png" /></p>
<p><img alt="image-20241030下午75540447-0289341" src="./imgs/./imgs/06、爬虫入门与urllib&amp;requests.assets/image-20241030下午75540447-0289341.png" /></p>
<h2 id="_4">四、浏览器工具的使用(重点)</h2>
<p>Chrome是一款非常优秀的浏览器. 不仅仅体现在用户使用上. 对于我们开发人员而言也是非常非常好用的. </p>
<p>对于一名爬虫工程师而言. 浏览器是最能直观的看到网页情况以及网页加载内容的地方. 我们可以按下F12来查看一些普通用户很少能使用到的工具. </p>
<p><img alt="image-20210519194028187" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20210519194028187.png" /></p>
<p>其中, 最重要的Elements, Console, Sources, Network. </p>
<p>Elements是我们实时的网页内容情况, 注意, 很多兄弟尤其到了后期. 非常容易混淆Elements以及页面源代码之间的关系. </p>
<blockquote>
<p>注意,  </p>
<ol>
<li>页面源代码是执行js脚本以及用户操作之前的服务器返回给我们最原始的内容</li>
<li>Elements中看到的内容是js脚本以及用户操作之后的当时的页面显示效果. </li>
</ol>
</blockquote>
<p>你可以理解为, 一个是老师批改之前的卷子, 一个是老师批改之后的卷子. 虽然都是卷子. 但是内容是不一样的. 而我们目前能够拿到的都是页面源代码. 也就是老师批改之前的样子. 这一点要格外注意. </p>
<p>在Elements中我们可以使用左上角的小箭头.可以直观的看到浏览器中每一块位置对应的当前html状况. 还是很贴心的. </p>
<p><img alt="image-20210519194515866" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20210519194515866.png" /></p>
<p>第二个窗口, Console是用来查看程序员留下的一些打印内容, 以及日志内容的. 我们可以在这里输入一些js代码自动执行. </p>
<p><img alt="image-20210519194811565" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20210519194811565.png" /></p>
<p>等咱们后面讲解js逆向的时候会用到这里.</p>
<p>第三个窗口, Source, 这里能看到该网页打开时加载的所有内容. 包括页面源代码. 脚本. 样式, 图片等等全部内容. </p>
<p><img alt="image-20210519195035084" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20210519195035084.png" /></p>
<p>第四个窗口, Network, 我们一般习惯称呼它为抓包工具. 在这里, 我们能看到当前网页加载的所有网路网络请求, 以及请求的详细内容. 这一点对我们爬虫来说至关重要. </p>
<p><img alt="image-20210519195221734" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20210519195221734.png" /></p>
<p><img alt="image-20210519195336616" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20210519195336616.png" /></p>
<p><img alt="image-20210519195502709" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20210519195502709.png" /></p>
<p><img alt="image-20210519195613396" src="imgs/06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.assets/image-20210519195613396.png" /></p>
<p>其他更加具体的内容. 随着咱们学习的展开. 会逐一进行讲解. </p>
<h2 id="http">五、http协议</h2>
<h3 id="1_2">1、 什么是协议？双方规定的传输形式</h3>
<ul>
<li>概述</li>
</ul>
<p>超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的WWW文件都必须遵守这个标准</p>
<ul>
<li>
<p>http协议：网站原理</p>
</li>
<li>
<p>应用层的协议  ftp(21)</p>
</li>
<li>常见端口号</li>
<li>http(80)</li>
<li>https(443)   </li>
<li>ssh(22)  </li>
<li>mysql(3306)  </li>
<li>redis(6379)</li>
<li>mongo(27017)</li>
</ul>
<h3 id="2httpshttp">2、HTTPS和HTTP的主要区别</h3>
<p><code>https://www.cnblogs.com/wqhwe/p/5407468.html</code></p>
<ul>
<li>https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。</li>
<li>http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。</li>
<li>http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。</li>
<li>http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。</li>
</ul>
<h3 id="3_2">3、请求包含</h3>
<p>http协议详解
https://www.cnblogs.com/10158wsj/p/6762848.html</p>
<ul>
<li>包含</li>
</ul>
<p>请求行、请求头、请求内容</p>
<ul>
<li>请求行：
        get、post，以及区别</li>
<li>常见请求头：</li>
<li>accept:浏览器通过这个头告诉服务器，它所支持的数据类型</li>
<li>Accept-Charset: 浏览器通过这个头告诉服务器，它支持哪种字符集</li>
<li>Accept-Encoding：浏览器通过这个头告诉服务器，支持的压缩格式</li>
<li>Accept-Language：浏览器通过这个头告诉服务器，它的语言环境</li>
<li>Host：浏览器通过这个头告诉服务器，想访问哪台主机</li>
<li>If-Modified-Since: 浏览器通过这个头告诉服务器，缓存数据的时间</li>
<li>Referer：浏览器通过这个头告诉服务器，客户机是哪个页面来的  防盗链</li>
<li>Connection：浏览器通过这个头告诉服务器，请求完后是断开链接还是何持链接</li>
<li>X-Requested-With: XMLHttpRequest  代表通过ajax方式进行访问</li>
<li>http响应头部信息</li>
<li>Location: 服务器通过这个头，来告诉浏览器跳到哪里</li>
<li>Server：服务器通过这个头，告诉浏览器服务器的型号</li>
<li>Content-Encoding：服务器通过这个头，告诉浏览器，数据的压缩格式</li>
<li>Content-Length: 服务器通过这个头，告诉浏览器回送数据的长度</li>
<li>Content-Language: 服务器通过这个头，告诉浏览器语言环境</li>
<li>Content-Type：服务器通过这个头，告诉浏览器回送数据的类型</li>
<li>Refresh：服务器通过这个头，告诉浏览器定时刷新</li>
<li>Content-Disposition: 服务器通过这个头，告诉浏览器以下载方式打数据</li>
<li>Transfer-Encoding：服务器通过这个头，告诉浏览器数据是以分块方式回送的</li>
<li>Expires: -1  控制浏览器不要缓存</li>
<li>Cache-Control: no-cache  </li>
<li>Pragma: no-cache</li>
</ul>
<h3 id="4http">4、常见HTTP状态码</h3>
<ul>
<li>
<p>100：这个状态码是告诉客户端应该继续发送请求，这个临时响应是用来通知客户端的，部分的请求服务器已经接受，但是客户端应继续发送求请求的剩余部分，如果请求已经完成，就忽略这个响应，而且服务器会在请求完成后向客户发送一个最终的结果</p>
</li>
<li>
<p>200：这个是最常见的http状态码，表示服务器已经成功接受请求，并将返回客户端所请求的最终结果</p>
</li>
<li>
<p>202：表示服务器已经接受了请求，但是还没有处理，而且这个请求最终会不会处理还不确定</p>
</li>
<li>
<p>204：服务器成功处理了请求，但没有返回任何实体内容 ，可能会返回新的头部元信息</p>
</li>
<li>
<p>301：客户端请求的网页已经永久移动到新的位置，当链接发生变化时，返回301代码告诉客户端链接的变化，客户端保存新的链接，并向新的链接发出请求，已返回请求结果</p>
</li>
<li>
<p>404：请求失败，客户端请求的资源没有找到或者是不存在</p>
</li>
<li>
<p>500：服务器遇到未知的错误，导致无法完成客户端当前的请求。</p>
</li>
<li>
<p>503：服务器由于临时的服务器过载或者是维护，无法解决当前的请求，以上http状态码是服务器经常返回的状态代码，用户只能通过浏览器的状态了解服务器是否正常运行，一般除了错误的状态码，都不会看到服务器的状态码的，新SEOer你们了解到了吗？内容编辑来自51特色购SEO优化人员，想了解更过状态码的知识可以加我好友，一起相互交流学习</p>
</li>
</ul>
<h2 id="_5">六、爬虫准备工作</h2>
<h3 id="1-fiddler">1、 fiddler</h3>
<p>一个网页的呈现，中间不止一次http请求，平均一个网页差不多10-15个http请求</p>
<ul>
<li>配置https</li>
</ul>
<p>点击Tools--&gt;options---&gt;https---&gt;选中面板下</p>
<p>Capture Https CONNECTS</p>
<p>Decrypt Https Traffic</p>
<p>Ignore</p>
<p>复选框后，将Fiddler重启即可</p>
<ul>
<li>
<p>fiddler界面介绍</p>
</li>
<li>
<p>Fiddler菜单栏</p>
<p>包括捕获http请求，停止捕获请求，保存http请求，载入本地session、设置捕获规则等功能</p>
</li>
<li>
<p>Fiddler的工具栏</p>
<p>包括Fiddler针对当前view的操作（暂停，清除session,decode模式、清除缓存等）</p>
</li>
<li>
<p>Web Session面板</p>
<p>主要是Fiddler抓取到的每条http请求（每一条称为一个session）,主要包含了请求的url，协议，状态码，body等信息</p>
</li>
<li>
<p>详情和数据统计板</p>
<p>针对每条http请求的具体统计（例如发送/接受字节数，发送/接收时间，还有粗略统计世界各地访问该服务器所花费的时间）和数据包分析。如inspector面板下，提供headers、textview、hexview,Raw等多种方式查看单条http请求的请求报文的信息</p>
<ul>
<li>Inspector</li>
</ul>
<p>提供供headers、textview、hexview,Raw等多种方式查看单条http请求的请求报文的信息,分为上下两个部分，上半部分是请求头部分，下半部分是响应头部分。</p>
<ul>
<li>ImageView标签 </li>
</ul>
<p>JPG 格式使用 ImageView 就可以看到图片</p>
<ul>
<li>TextView 标签</li>
</ul>
<p>HTML/JS/CSS 使用 TextView 可以看到响应的内容。选择一条Content-Type是text/html的会话</p>
<ul>
<li>Raw标签</li>
</ul>
<p>Raw标签可以查看响应报文和响应正文,但是不包含请求报文</p>
<ul>
<li>Cookies标签</li>
</ul>
<p>Cookies标签可以看到请求的cookie和响应的set-cookie头信息。</p>
<ul>
<li>WebForms</li>
</ul>
<p>post请求所有表单数据</p>
<ul>
<li>Headers</li>
</ul>
<p>请求头和响应头信息</p>
<ul>
<li>Json\XML</li>
</ul>
<p>Json或XML格式的数据</p>
<ul>
<li>Statistics面板</li>
</ul>
<p>HTTP请求的性能和其他数据分析</p>
<ul>
<li>composer面板</li>
</ul>
<p>可以模拟向相应的服务器发送数据的过程</p>
<ul>
<li>Filters面板</li>
</ul>
<p>Filter标签则可以设置Fiddler的过滤规则，来达到过滤http请求的目的。最简单如：过滤内网http请求而只抓取internet的http请求，或则过滤相应域名的http请求。Fiddler的过滤器非常强大，可以过滤特定http状态码的请求，可以过滤特定请求类型的http请求（如css请求，image请求，js请求等），可以过滤请求报文大于或则小于指定大小（byte）的请求</p>
</li>
<li>
<p>关闭fiddler</p>
</li>
</ul>
<p>左上角File,反选Capture Traffic</p>
<ul>
<li>WebSession过滤功能</li>
</ul>
<p>点击工具栏中的X符号</p>
<ul>
<li>WebSession选择功能</li>
</ul>
<p>左下角的黑色对话框</p>
<ul>
<li>select json\html\image</li>
<li>cls清除所有请求</li>
<li>
<p>?xxx搜索</p>
</li>
<li>
<p>谷歌浏览器</p>
</li>
<li>
<p>mac 青花瓷</p>
</li>
</ul>
<p>mac版安装：http://www.cocoachina.com/apple/20170704/19729.html</p>


</section>
</div> <!-- end of search-noresults -->
<div class="search-results">
<div class="has-results">

<h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
<ul class="search-results-list"></ul>

</div> <!-- end of has-results -->
<div class="no-results">

<h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>

</div> <!-- end of no-results -->
</div> <!-- end of search-results -->
</div> <!-- end of book-search-results -->

</div> <!-- end of page-inner -->
</div> <!-- end of page-wrapper -->

</div> <!-- end of body-inner -->

</div> <!-- end of book-body -->
<script src="./js/main.js"></script>
<script src="search/main.js"></script>
<script src="./js/gitbook.min.js"></script>
<script src="./js/theme.min.js"></script>
</body>
</html>