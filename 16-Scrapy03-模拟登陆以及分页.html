<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<title>十六、scrapy模拟登陆&amp;分页 - js node</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="generator" content="mkdocs-1.6.1, mkdocs-gitbook-1.0.7">

<link rel="shortcut icon" href="./images/favicon.ico" type="image/x-icon">
<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta rel="next" href="" />
<link href="./css/style.min.css" rel="stylesheet"> 
</head>

<body>
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input type="text" placeholder="Type to search" />
</div> <!-- end of book-search-input -->

<nav role="navigation">
<ul class="summary">
<li>
<a href="index.html" target="_blank" class="custom-link">js node</a>
</li>
<li class="divider"></li>
<li class="chapter" data-path="index.html">
<a href="index.html">Welcome to MkDocs</a>
<li class="chapter" data-path="01%E3%80%81html.html">
<a href="01%E3%80%81html.html">一、HTML</a>
<li class="chapter" data-path="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">
<a href="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">二、CSS层叠样式表</a>
<li class="chapter" data-path="03%E3%80%81%E6%AD%A3%E5%88%99.html">
<a href="03%E3%80%81%E6%AD%A3%E5%88%99.html">三、正则</a>
<li class="chapter" data-path="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">
<a href="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">四、beautifulsoup</a>
<li class="chapter" data-path="05%E3%80%81xpath.html">
<a href="05%E3%80%81xpath.html">五、xpath</a>
<li class="chapter" data-path="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">
<a href="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">六、爬虫入门</a>
<li class="chapter" data-path="07%E3%80%81urllib%E4%B8%8Erequests.html">
<a href="07%E3%80%81urllib%E4%B8%8Erequests.html">七、urllib与requests</a>
<li class="chapter" data-path="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">
<a href="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">八、多进程</a>
<li class="chapter" data-path="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">
<a href="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">九、线程</a>
<li class="chapter" data-path="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">
<a href="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">十、携程</a>
<li class="chapter" data-path="11%E3%80%81selenium.html">
<a href="11%E3%80%81selenium.html">十一、selenium</a>
<li class="chapter" data-path="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">
<a href="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">十二、MySQL数据库</a>
<li class="chapter" data-path="13%E3%80%81Mongodb.html">
<a href="13%E3%80%81Mongodb.html">十三、Mongodb</a>
<li class="chapter" data-path="14%E3%80%81redis.html">
<a href="14%E3%80%81redis.html">十四、Redis数据库</a>
<li class="chapter" data-path="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">
<a href="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">十五、面向对象</a>
<li class="chapter" data-path="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">
<a href="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">十六、Scrapy框架初认识</a>
<li class="chapter" data-path="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">
<a href="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">十六、Scrapy深入使用-存储</a>
<li class="chapter active" data-path="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">
<a href="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">十六、scrapy模拟登陆&amp;分页</a>
<li class="chapter" data-path="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">
<a href="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">十六、Scrapy中间件</a>
<li class="chapter" data-path="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">
<a href="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">十六、scrapy的crawlspider爬虫</a>
<li class="chapter" data-path="16-Scrapy06-scrapy_redis.html">
<a href="16-Scrapy06-scrapy_redis.html">十六、scrapy_redis</a>
<li class="chapter" data-path="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">
<a href="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">十七、js数据类型</a>
<li class="chapter" data-path="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">
<a href="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">十八、js运算符与流程控制</a>
<li class="chapter" data-path="19%E3%80%81js%E6%95%B0%E7%BB%84.html">
<a href="19%E3%80%81js%E6%95%B0%E7%BB%84.html">十九、js数组</a>
<li class="chapter" data-path="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">
<a href="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">二十、js字符串</a>
<li class="chapter" data-path="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">
<a href="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">二十一、js对象与时间</a>
<li class="chapter" data-path="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">
<a href="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">二十二、js函数</a>
<li class="chapter" data-path="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">
<a href="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">二十三、Javascript进阶</a>
<li class="chapter" data-path="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">
<a href="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">二十四、浏览器对象模型BOM</a>
<li class="chapter" data-path="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">
<a href="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">二十五、DOM操作</a>
<li class="chapter" data-path="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">
<a href="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">二十六、jQuery</a>
<li class="chapter" data-path="27%E3%80%81%E9%80%86%E5%90%9101.html">
<a href="27%E3%80%81%E9%80%86%E5%90%9101.html">二十七、JS逆向01</a>
<li class="chapter" data-path="28%E3%80%81%E9%80%86%E5%90%9102.html">
<a href="28%E3%80%81%E9%80%86%E5%90%9102.html">二十八、逆向02</a>
<li class="chapter" data-path="29%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">
<a href="29%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">二十九、微信小程序逆向开发</a>
<li class="chapter" data-path="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">
<a href="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">三十、网易易盾</a>
<li class="chapter" data-path="31%E3%80%81rpc.html">
<a href="31%E3%80%81rpc.html">三十一、RPC</a>
<li class="chapter" data-path="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">
<a href="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">三十二、TLS指纹绕过</a>
<li class="chapter" data-path="33%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">
<a href="33%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">三十三、高级逆向</a>
<li class="chapter" data-path="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">
<a href="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">三十四、高级逆向</a>
<li class="chapter" data-path="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">
<a href="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">逆向实战案例笔记</a>
<li class="header">Pyexecjs与npm配置</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/execjs%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8.html" class="">Execjs介绍以及安装和使用</a>
</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/%E4%B8%80%E6%AC%A1%E6%80%A7%E8%A7%A3%E5%86%B3%E6%8E%89npm%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98.html" class="">更换npm为国内镜像</a>
</li>

<li class="header">补环境</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/1%E3%80%81%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%8E%E4%BA%8B%E4%BB%B6.html" class="">补环境</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/2%E3%80%81Proxy%E4%BB%A3%E7%90%86%E5%99%A8.html" class="">Proxy代理</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/3%E3%80%81vm2%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83.html" class="">3、vm2运行环境</a>
</li>

<li class="divider"></li>



<li><a href="http://www.mkdocs.org">
Published with MkDocs
</a></li>

<li><a href="https://github.com/GitbookIO/theme-default">
Theme by GitBook
</a></li>
</ul>

</nav>

</div> <!-- end of book-summary -->

<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">

<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="." ></a>
</h1>

</div> <!-- end of book-header -->

<div class="page-wrapper" tabindex="-1" role="main">
<div class="page-inner">
<div id="book-search-results">
<div class="search-noresults">

<section class="normal markdown-section">



<h1 id="scrapy">十六、scrapy模拟登陆&amp;分页</h1>
<h2 id="_1">一、模拟登陆</h2>
<h5 id="_2">学习目标：</h5>
<ol>
<li>应用 scrapy直接携带cookie模拟登陆的方法</li>
<li>应用 scrapy.FormRequest()发送post请求进行登陆</li>
<li>应用 scrapy.FormRequest.from_response()发送表单请求</li>
</ol>
<hr />
<h3 id="1">1、回顾之前的模拟登陆的方法</h3>
<h5 id="11-requests">1.1 requests模块是如何实现模拟登陆的？</h5>
<ol>
<li>直接携带cookies请求页面</li>
<li>找url地址，发送post请求存储cookie</li>
</ol>
<h5 id="12-selenium">1.2 selenium是如何模拟登陆的？</h5>
<ol>
<li>找到对应的input标签，输入文本点击登陆</li>
</ol>
<h5 id="13-scrapy">1.3 scrapy有三种方法模拟登陆</h5>
<ol>
<li>直接携带cookies</li>
<li>找url地址，发送post请求存储cookie</li>
<li>找到对应的form表单，自动解析input标签，自动解析post请求的url地址，自动带上数据，自动发送请求</li>
</ol>
<h3 id="2scrapycookies">2、scrapy携带cookies直接获取需要登陆后的页面</h3>
<p><strong>17k小说网</strong></p>
<pre><code>https://user.17k.com/
</code></pre>
<h5 id="21">2.1 应用场景</h5>
<ol>
<li>cookie过期时间很长，常见于一些不规范的网站</li>
<li>能在cookie过期之前把搜有的数据拿到</li>
<li>配合其他程序使用，比如其使用selenium把登陆之后的cookie获取到保存到本地，scrapy发送请求之前先读取本地cookie</li>
</ol>
<h5 id="22-settingsdefault_request_headerscookie">2.2 通过修改settings中DEFAULT_REQUEST_HEADERS携带cookie</h5>
<p>settings.py</p>
<pre><code class="language-python">DEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36',
  'Cookie': 'ASP.NET_SessionId=n4lwamv5eohaqcorfi3dvzcv; xiaohua_visitfunny=103174; xiaohua_web_userid=120326; xiaohua_web_userkey=r2sYazfFMt/rxUn8LJDmUYetwR2qsFCHIaNt7+Zpsscpp1p6zicW4w=='
}
</code></pre>
<p>注意：需要打开COOKIES_ENABLED，否则上面设定的cookie将不起作用</p>
<pre><code class="language-python"># Disable cookies (enabled by default)
COOKIES_ENABLED = False
</code></pre>
<p><strong>特别说明</strong></p>
<ul>
<li>当COOKIES_ENABLED是注释的时候scrapy默认没有开启cookie</li>
<li>当COOKIES_ENABLED没有注释，设置为False的时候scrapy默认使用了settings里面的cookie</li>
<li>当COOKIES_ENABLED设置为True的时候scrapy就会把settings的cookie关掉，使用自定义cookie</li>
<li>
<p>也就是</p>
<p>如果使用自定义cookie就把COOKIES_ENABLED设置为True</p>
<p>如果使用settings的cookie就把COOKIES_ENABLED设置为False</p>
</li>
</ul>
<p><strong>xiaoshuo.py</strong></p>
<pre><code class="language-python">import scrapy


class DengluSpider(scrapy.Spider):
    name = 'denglu'
    allowed_domains = ['17k.com']
    start_urls = ['https://user.17k.com/ck/user/mine/readList?page=1&amp;appKey=2406394919']

    def parse(self, res):
        print(res.text)
</code></pre>
<p><strong>局限性：</strong></p>
<p>当前设定方式虽然可以实现携带cookie保持登录，但是无法获取到新cookie，也就是当前cookie一直是固定的</p>
<p>如果cookie是经常性变化，那么当前不适用</p>
<h5 id="23-scrapystarte_rquests">2.3 实现：重构scrapy的starte_rquests方法</h5>
<p>scrapy中start_url是通过start_requests来进行处理的，其实现代码如下</p>
<pre><code class="language-python">def start_requests(self):
    cls = self.__class__
    if method_is_overridden(cls, Spider, 'make_requests_from_url'):
        warnings.warn(
            &quot;Spider.make_requests_from_url method is deprecated; it &quot;
            &quot;won't be called in future Scrapy releases. Please &quot;
            &quot;override Spider.start_requests method instead (see %s.%s).&quot; % (
                cls.__module__, cls.__name__
            ),
        )
        for url in self.start_urls:
            yield self.make_requests_from_url(url)
    else:
        for url in self.start_urls:
            yield Request(url, dont_filter=True)
</code></pre>
<p>所以对应的，如果start_url地址中的url是需要登录后才能访问的url地址，则需要重写start_request方法并在其中手动添加上cookie</p>
<p>settings.py</p>
<pre><code class="language-python">import scrapy


class DengluSpider(scrapy.Spider):
    name = 'denglu'
    # allowed_domains = ['https://user.17k.com/ck/user/mine/readList?page=1']
    start_urls = ['https://user.17k.com/ck/user/mine/readList?page=1&amp;appKey=2406394919']

    def start_requests(self):
        cookies = 'GUID=796e4a09-ba11-4ecb-9cf6-aad19169267d; Hm_lvt_9793f42b498361373512340937deb2a0=1660545196; c_channel=0; c_csc=web; accessToken=avatarUrl%3Dhttps%253A%252F%252Fcdn.static.17k.com%252Fuser%252Favatar%252F18%252F98%252F90%252F96139098.jpg-88x88%253Fv%253D1650527904000%26id%3D96139098%26nickname%3D%25E4%25B9%25A6%25E5%258F%258BqYx51ZhI1%26e%3D1677033668%26s%3D8e116a403df502ab; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2296139098%22%2C%22%24device_id%22%3A%22181d13acb2c3bd-011f19b55b75a8-1c525635-1296000-181d13acb2d5fb%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%7D%2C%22first_id%22%3A%22796e4a09-ba11-4ecb-9cf6-aad19169267d%22%7D; Hm_lpvt_9793f42b498361373512340937deb2a0=1661483362'
        cookie_dic = {}
        for i in cookies.split(';'):
            v = i.split('=')
            cookie_dic[v[0]] = v[1]
        # {i.split('=')[0]:i.split('=')[1] for i in cookies_str.split('; ')}   # 简写
        for url in self.start_urls:
            yield scrapy.Request(url, cookies=cookie_dic)


    def parse(self, response):
        print(response.text)
</code></pre>
<h5 id="_3">注意：</h5>
<ol>
<li>scrapy中cookie不能够放在headers中，在构造请求的时候有专门的cookies参数，能够接受字典形式的coookie</li>
<li>在setting中设置ROBOTS协议、USER_AGENT</li>
<li>当前方式不需要对settings中的 COOKIES_ENABLED 有任何操作，直接默认注释的就可以</li>
</ol>
<h3 id="3scrapyformrequestpost">3、scrapy.FormRequest发送post请求</h3>
<blockquote>
<p>我们知道可以通过scrapy.Request()指定method、body参数来发送post请求；那么也可以使用scrapy.FormRequest()来发送post请求</p>
</blockquote>
<h5 id="31-scrapyformrequest">3.1 scrapy.FormRequest()的使用</h5>
<p>通过scrapy.FormRequest能够发送post请求，同时需要添加fromdata参数作为请求体，以及callback</p>
<pre><code class="language-python">login_url = 'https://passport.17k.com/ck/user/login'      
yield scrapy.FormRequest(
            url=login_url, 
            formdata={'loginName': '17346570232', 'password': 'xlg17346570232'}, 
            callback=self.do_login
)
</code></pre>
<h4 id="32-scrapyformrequest">3.2 使用scrapy.FormRequest()登陆</h4>
<h5 id="321">3.2.1 思路分析</h5>
<ol>
<li>找到post的url地址：点击登录按钮进行抓包，然后定位url地址为https://passport.17k.com/ck/user/login</li>
<li>找到请求体的规律：分析post请求的请求体，其中包含的参数均在前一次的响应中</li>
<li>否登录成功：通过请求个人主页，观察是否包含用户名</li>
</ol>
<h5 id="322">3.2.2 代码实现如下：</h5>
<pre><code class="language-python">import scrapy


class DengluSpider(scrapy.Spider):
    name = 'denglu'
    # allowed_domains = ['17k.com']
    start_urls = ['https://user.17k.com/ck/user/mine/readList?page=1&amp;appKey=2406394919']

    def start_requests(self):
        '''
        请求登陆的账号和密码
        '''
        login_url = 'https://passport.17k.com/ck/user/login'
        # 使用request进行请求
        # yield scrapy.Request(url=login_url, body='loginName=17346570232&amp;password=xlg17346570232', callback=self.do_login, method='POST', headers={&quot;Content-Type&quot;:
&quot;application/x-www-form-urlencoded; charset=UTF-8&quot;})
        # 使用Request子类FormRequest进行请求  自动为post请求
        yield scrapy.FormRequest(
            url=login_url,
            formdata={'loginName': '17346570232', 'password': 'xlg17346570232'},
            callback=self.do_login
        )

    def do_login(self, response):
        '''
        登陆成功后调用parse进行处理
        cookie中间件会帮我们自动处理携带cookie
        '''
        for url in self.start_urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response, **kwargs):
        print(response.text)
</code></pre>
<h3 id="4scrapy">4、scrapy自动提交表单（没尝试出来目前）</h3>
<h5 id="41-scrapyformrequestfrom_response">4.1 scrapy.Formrequest.from_response</h5>
<p>它能够自动的从响应中寻找form表单，然后把formdata中的数据提交到action对应的url地址中</p>
<pre><code class="language-python">yield scrapy.FormRequest.from_response(
    response, # 传入response对象,自动解析
    # 可以通过xpath来定位form表单,当前页只有一个form表单时,将会自动定位
    formxpath='//*[@id=&quot;login&quot;]/form',  # 可以不写
    formdata={'login': 'noobpythoner', 'password': '***'},
    callback=self.parse_login
)
</code></pre>
<h5 id="42-scrapyformrequestfrom_responsegithub">4.2 使用scrapy.Formrequest.from_response登陆github</h5>
<pre><code class="language-python">import scrapy
import re

class Login3Spider(scrapy.Spider):
    name = 'login3'
    allowed_domains = ['github.com']
    start_urls = ['https://github.com/login']

    def parse(self, response):
        yield scrapy.FormRequest.from_response(
            response, # 传入response对象,自动解析
            # 可以通过xpath来定位form表单,当前页只有一个form表单时,将会自动定位
            formxpath='//*[@id=&quot;login&quot;]/form', 
            formdata={'login': 'noobpythoner', 'password': 'zhoudawei123'},
            callback=self.parse_login
        )

    def parse_login(self,response):
        ret = re.findall(r&quot;noobpythoner|NoobPythoner&quot;, response.text)
        print(ret)
</code></pre>
<h3 id="5">5、小技巧</h3>
<p>在settings.py中通过设置COOKIES_DEBUG=TRUE 能够在终端看到cookie的传递传递过程</p>
<p>注意关闭LOG_LEVEL </p>
<p><img alt="scrapy-login-1" src="imgs/16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.assets/scrapy-login-1.png" /></p>
<h3 id="_4">总结</h3>
<ol>
<li>start_urls中的url地址是交给start_request处理的，如有必要，可以重写start_request函数</li>
<li>直接携带cookie登陆：cookie只能传递给cookies参数接收</li>
<li>scrapy.FormRequest()发送post请求</li>
<li>scrapy.FormRequest.from_response()发送表单请求，接收的是response</li>
</ol>
<h3 id="_5">练习：</h3>
<p>笑话网：https://www.xiaohua.com/</p>
<h2 id="scrapy_1">二、scrapy发送翻页请求</h2>
<h5 id="_6">学习目标：</h5>
<ol>
<li>应用 完善并使用Item数据类</li>
<li>应用 构造Request对象，并发送请求</li>
<li>应用 利用meta参数在不同的解析函数中传递数据</li>
</ol>
<hr />
<h3 id="1_1">1、翻页请求的思路</h3>
<p>对于要提取如下图中所有页面上的数据该怎么办？</p>
<p><img alt="scrapy翻页" src="imgs/16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.assets/scrapy%E7%BF%BB%E9%A1%B5.png" /></p>
<p>回顾requests模块是如何实现翻页请求的：</p>
<ol>
<li>找到下一页的URL地址</li>
<li>调用requests.get(url)</li>
</ol>
<p>scrapy实现翻页的思路：</p>
<ol>
<li>找到下一页的url地址</li>
<li>构造url地址的请求，传递给引擎</li>
</ol>
<h3 id="2scrapy">2、scrapy实现翻页请求</h3>
<h4 id="21_1">2.1 实现方法</h4>
<ol>
<li>确定url地址</li>
<li>构造请求，scrapy.Request(url,callback)</li>
<li>callback：指定解析函数名称，表示该请求返回的响应使用哪一个函数进行解析</li>
<li>把请求交给引擎：yield scrapy.Request(url,callback)</li>
</ol>
<h4 id="22">2.2 段子/豆瓣电影爬虫</h4>
<blockquote>
<p>通过爬取段子页面的信息,学习如何实现翻页请求</p>
<p>地址：https://duanzixing.com/</p>
</blockquote>
<h5 id="_7">思路分析：</h5>
<ol>
<li>获取首页的数据</li>
<li>寻找下一页的地址，进行翻页，获取数据</li>
</ol>
<h5 id="_8">注意：</h5>
<ol>
<li>可以在settings中设置ROBOTS协议</li>
</ol>
<p><code>python
   # False表示忽略网站的robots.txt协议，默认为True
   ROBOTSTXT_OBEY = False</code></p>
<ol>
<li>可以在settings中设置User-Agent：</li>
</ol>
<p><code>python
   # scrapy发送的每一个请求的默认UA都是设置的这个User-Agent
   USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'</code></p>
<h4 id="23">2.3 代码实现</h4>
<ul>
<li>段子实现多页抓取</li>
</ul>
<p>```python
  import scrapy
  from duanzi01.items import Duanzi01Item</p>
<p>class DzSpider(scrapy.Spider):
      name = 'dz'
      allowed_domains = ['duanzixing.com']
      start_urls = ['http://duanzixing.com/']</p>
<pre><code>  def parse(self, response, **kwargs):
      # 多页的url
      url = 'https://duanzixing.com/page/%d/'
      for i in range(1, 10):
          # 拼接新的url
          new_url = url % i
          yield scrapy.Request(new_url, callback=self.get_data, meta={'url': new_url})

  def get_data(self, response, **kwargs):
      article_list = response.xpath('/html/body/section/div/div/article')
      for article in article_list:
          item = Duanzi01Item()
          title = article.xpath('./header/h2/a/text()').extract_first()
          con = article.xpath('./p[2]/text()').extract_first()
          item['title'] = title
          item['con'] = con
          print(response.meta['url'], item)
          yield item
</code></pre>
<p>```</p>
<p>items.py</p>
<p>```python
  import scrapy</p>
<p>class XiaoshuoItem(scrapy.Item):
      title = scrapy.Field()
      con = scrapy.Field()
  ```</p>
<ul>
<li>豆瓣电影抓取子页面数据(手动调用请求练习)</li>
</ul>
<p>新建豆瓣项目</p>
<p>scrapy startproject douban</p>
<p>cd douban</p>
<p>scrapy genspider db</p>
<p>scrapy crawl db</p>
<p>```python
  import scrapy</p>
<p>class DbSpider(scrapy.Spider):
      name = 'db'
      # allowed_domains = ['www.xxx.com']
      start_urls = ['https://movie.douban.com/chart']</p>
<pre><code>  def parse(self, response):
      table_list = response.xpath('//*[@id="content"]/div/div[1]/div/div/table')
      for table in table_list:
          # 抓取子页面url地址
          href = table.xpath('./tr/td[2]/div/a/@href').extract_first()
          print(href)
          # 手动请求子页面
          yield scrapy.Request(href, callback=self.parse_deatil)

  # 解析子页面数据
  def parse_deatil(self, response):
      # 获取所有的电影信息
      print(response.xpath('//*[@id="info"]//text()').extract())
</code></pre>
<p>```</p>
<h4 id="24-scrapyrequest">2.4 scrapy.Request的更多参数</h4>
<pre><code class="language-python">scrapy.Request(url[,callback,method=&quot;GET&quot;,headers,body,cookies, 
meta,dont_filter=False])
</code></pre>
<h5 id="_9">参数解释</h5>
<ol>
<li>中括号中的参数为可选参数</li>
<li>callback：表示当前的url的响应交给哪个函数去处理</li>
<li>meta：实现数据在不同的解析函数中传递，meta默认带有部分数据，比如下载延迟，请求深度等</li>
<li>dont_filter:默认为False，会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动</li>
<li>method：指定POST或GET请求</li>
<li>headers：接收一个字典，其中不包括cookies</li>
<li>cookies：接收一个字典，专门放置cookies</li>
<li>body：接收一个字典，为POST的数据</li>
</ol>
<h3 id="3meta">3、meta参数的使用</h3>
<h5 id="meta">meta的形式:字典</h5>
<h5 id="metameta">meta的作用：meta可以实现数据在不同的解析函数中的传递</h5>
<p>在爬虫文件的parse方法中，提取详情页增加之前callback指定的parse_detail函数：</p>
<pre><code class="language-python">def parse(self,response):
    ...
    yield scrapy.Request(detail_url, callback=self.parse_detail,meta={&quot;item&quot;:item})
...

def parse_detail(self,response):
    #获取之前传入的item
    item = resposne.meta[&quot;item&quot;]
</code></pre>
<h5 id="_10">特别注意</h5>
<ol>
<li>meta参数是一个字典</li>
<li>meta字典中有一个固定的键<code>proxy</code>，表示代理ip，关于代理ip的使用我们将在scrapy的下载中间件的学习中进行介绍</li>
</ol>
<h3 id="4itemspy">4、items.py的使用</h3>
<h4 id="41-item">4.1 Item能够做什么</h4>
<ul>
<li>
<p>定义item即提前规划好哪些字段需要抓取，scrapy.Field()仅仅是提前占坑，通过item.py能够让别人清楚自己的爬虫是在抓取什么，同时定义好哪些字段是需要抓取的，没有定义的字段不能使用，防止手误</p>
</li>
<li>
<p>在python大多数框架中，大多数框架都会自定义自己的数据类型(在python自带的数据结构基础上进行封装)，目的是增加功能，增加自定义异常</p>
</li>
</ul>
<h4 id="42-item">4.2 定义Item</h4>
<p>在items.py文件中定义要提取的字段：</p>
<pre><code class="language-python">import scrapy

class DoubanItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    name = scrapy.Field()     # 电影名称
    director = scrapy.Field()     # 导演
    screenwriter = scrapy.Field()    # 编剧
    to_star = scrapy.Field()    # 主演
    type = scrapy.Field()    # 类型
    producer_country = scrapy.Field()    # 制片国家/地区:
    language = scrapy.Field()    # 语言
</code></pre>
<h4 id="43-item">4.3 使用Item</h4>
<p>Item使用之前需要先导入并且实例化，之后的使用方法和使用字典相同</p>
<p>修改爬虫文件db.py：</p>
<pre><code class="language-python">import scrapy
from douban.items import DoubanItem

class DbSpider(scrapy.Spider):
    name = 'db'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://movie.douban.com/chart']

    def parse(self, response):
        table_list = response.xpath('//*[@id=&quot;content&quot;]/div/div[1]/div/div/table')
        for table in table_list:
            # 抓取子页面url地址
            href = table.xpath('./tr/td[2]/div/a/@href').extract_first()
            print(href)
            # 手动请求子页面
            yield scrapy.Request(href, callback=self.parse_deatil)

    # 解析子页面数据
    def parse_deatil(self, response):
        # 提取电影详情
        item = DoubanItem()
        item['name'] = response.xpath('//*[@id=&quot;content&quot;]/h1/span[1]/text()').extract_first()  # 电影名称
        item['director'] = response.xpath('//*[@id=&quot;info&quot;]/span[1]/span[2]/a/text()').extract_first()  # 导演
        item['screenwriter'] = ''.join(response.xpath('//*[@id=&quot;info&quot;]/span[2]/span[2]//text()').extract())  # 编剧
        item['to_star'] = ''.join(response.xpath('//*[@id=&quot;info&quot;]/span[3]/span[2]//text()').extract())  # 主演
        item['type'] = '/'.join(response.xpath('//span[@property=&quot;v:genre&quot;]//text()').extract())  # 类型
        item['producer_country'] = response.xpath(&quot;//div[@id='info']/span[text()='制片国家/地区:']/following-sibling::text()[1]&quot;).extract_first()  # 制片国家/地区:
        item['language'] = response.xpath(&quot;//div[@id='info']/span[text()='语言:']/following-sibling::text()[1]&quot;).extract_first()  # 语言
        print(item)
        return item
</code></pre>
<h5 id="_11">注意：</h5>
<ol>
<li>from myspider.items import ITSpiderItem这一行代码中 注意item的正确导入路径，忽略pycharm标记的错误</li>
<li>python中的导入路径要诀：从哪里开始运行，就从哪里开始导入</li>
<li>在parse_deatil中item[key] 必须要和items.py中的key对应，否则抛出异常</li>
</ol>
<h3 id="_12">总结</h3>
<ol>
<li>完善并使用Item数据类：</li>
<li>在items.py中完善要爬取的字段</li>
<li>在爬虫文件中先导入Item</li>
<li>实力化Item对象后，像字典一样直接使用</li>
<li>构造Request对象，并发送请求：</li>
<li>导入scrapy.Request类</li>
<li>在解析函数中提取url</li>
<li>yield scrapy.Request(url, callback=self.parse_detail, meta={})</li>
<li>利用meta参数在不同的解析函数中传递数据:</li>
<li>通过前一个解析函数 yield scrapy.Request(url, callback=self.xxx, meta={}) 来传递meta</li>
<li>在self.xxx函数中 response.meta.get('key', '') 或 response.meta['key'] 的方式取出传递的数据</li>
</ol>


</section>
</div> <!-- end of search-noresults -->
<div class="search-results">
<div class="has-results">

<h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
<ul class="search-results-list"></ul>

</div> <!-- end of has-results -->
<div class="no-results">

<h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>

</div> <!-- end of no-results -->
</div> <!-- end of search-results -->
</div> <!-- end of book-search-results -->

</div> <!-- end of page-inner -->
</div> <!-- end of page-wrapper -->

</div> <!-- end of body-inner -->

</div> <!-- end of book-body -->
<script src="./js/main.js"></script>
<script src="search/main.js"></script>
<script src="./js/gitbook.min.js"></script>
<script src="./js/theme.min.js"></script>
</body>
</html>