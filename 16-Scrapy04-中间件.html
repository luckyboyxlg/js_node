<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<title>十六、Scrapy中间件 - js node</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="generator" content="mkdocs-1.6.1, mkdocs-gitbook-1.0.7">

<link rel="shortcut icon" href="./images/favicon.ico" type="image/x-icon">
<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta rel="next" href="" />
<link href="./css/style.min.css" rel="stylesheet"> 
</head>

<body>
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input type="text" placeholder="Type to search" />
</div> <!-- end of book-search-input -->

<nav role="navigation">
<ul class="summary">
<li>
<a href="index.html" target="_blank" class="custom-link">js node</a>
</li>
<li class="divider"></li>
<li class="chapter" data-path="index.html">
<a href="index.html">Welcome to MkDocs</a>
<li class="chapter" data-path="01%E3%80%81html.html">
<a href="01%E3%80%81html.html">一、HTML</a>
<li class="chapter" data-path="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">
<a href="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">二、CSS层叠样式表</a>
<li class="chapter" data-path="03%E3%80%81%E6%AD%A3%E5%88%99.html">
<a href="03%E3%80%81%E6%AD%A3%E5%88%99.html">三、正则</a>
<li class="chapter" data-path="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">
<a href="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">四、beautifulsoup</a>
<li class="chapter" data-path="05%E3%80%81xpath.html">
<a href="05%E3%80%81xpath.html">五、xpath</a>
<li class="chapter" data-path="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">
<a href="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">六、爬虫入门</a>
<li class="chapter" data-path="07%E3%80%81urllib%E4%B8%8Erequests.html">
<a href="07%E3%80%81urllib%E4%B8%8Erequests.html">七、urllib与requests</a>
<li class="chapter" data-path="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">
<a href="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">八、多进程</a>
<li class="chapter" data-path="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">
<a href="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">九、线程</a>
<li class="chapter" data-path="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">
<a href="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">十、携程</a>
<li class="chapter" data-path="11%E3%80%81selenium.html">
<a href="11%E3%80%81selenium.html">十一、selenium</a>
<li class="chapter" data-path="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">
<a href="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">十二、MySQL数据库</a>
<li class="chapter" data-path="13%E3%80%81Mongodb.html">
<a href="13%E3%80%81Mongodb.html">十三、Mongodb</a>
<li class="chapter" data-path="14%E3%80%81redis.html">
<a href="14%E3%80%81redis.html">十四、Redis数据库</a>
<li class="chapter" data-path="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">
<a href="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">十五、面向对象</a>
<li class="chapter" data-path="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">
<a href="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">十六、Scrapy框架初认识</a>
<li class="chapter" data-path="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">
<a href="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">十六、Scrapy深入使用-存储</a>
<li class="chapter" data-path="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">
<a href="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">十六、scrapy模拟登陆&amp;分页</a>
<li class="chapter active" data-path="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">
<a href="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">十六、Scrapy中间件</a>
<li class="chapter" data-path="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">
<a href="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">十六、scrapy的crawlspider爬虫</a>
<li class="chapter" data-path="16-Scrapy06-scrapy_redis.html">
<a href="16-Scrapy06-scrapy_redis.html">十六、scrapy_redis</a>
<li class="chapter" data-path="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">
<a href="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">十七、js数据类型</a>
<li class="chapter" data-path="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">
<a href="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">十八、js运算符与流程控制</a>
<li class="chapter" data-path="19%E3%80%81js%E6%95%B0%E7%BB%84.html">
<a href="19%E3%80%81js%E6%95%B0%E7%BB%84.html">十九、js数组</a>
<li class="chapter" data-path="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">
<a href="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">二十、js字符串</a>
<li class="chapter" data-path="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">
<a href="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">二十一、js对象与时间</a>
<li class="chapter" data-path="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">
<a href="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">二十二、js函数</a>
<li class="chapter" data-path="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">
<a href="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">二十三、Javascript进阶</a>
<li class="chapter" data-path="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">
<a href="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">二十四、浏览器对象模型BOM</a>
<li class="chapter" data-path="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">
<a href="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">二十五、DOM操作</a>
<li class="chapter" data-path="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">
<a href="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">二十六、jQuery</a>
<li class="chapter" data-path="27%E3%80%81%E9%80%86%E5%90%9101.html">
<a href="27%E3%80%81%E9%80%86%E5%90%9101.html">二十七、JS逆向01</a>
<li class="chapter" data-path="28%E3%80%81%E9%80%86%E5%90%9102.html">
<a href="28%E3%80%81%E9%80%86%E5%90%9102.html">二十八、逆向02</a>
<li class="chapter" data-path="29%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">
<a href="29%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">二十九、微信小程序逆向开发</a>
<li class="chapter" data-path="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">
<a href="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">三十、网易易盾</a>
<li class="chapter" data-path="31%E3%80%81rpc.html">
<a href="31%E3%80%81rpc.html">三十一、RPC</a>
<li class="chapter" data-path="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">
<a href="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">三十二、TLS指纹和JA3指纹绕过</a>
<li class="chapter" data-path="33%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">
<a href="33%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">三十三、高级逆向</a>
<li class="chapter" data-path="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">
<a href="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">三十四、高级逆向</a>
<li class="chapter" data-path="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">
<a href="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">逆向实战案例笔记</a>
<li class="header">Pyexecjs与npm配置</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/execjs%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8.html" class="">Execjs介绍以及安装和使用</a>
</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/%E4%B8%80%E6%AC%A1%E6%80%A7%E8%A7%A3%E5%86%B3%E6%8E%89npm%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98.html" class="">更换npm为国内镜像</a>
</li>

<li class="header">补环境</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/1%E3%80%81%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%8E%E4%BA%8B%E4%BB%B6.html" class="">补环境</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/2%E3%80%81Proxy%E4%BB%A3%E7%90%86%E5%99%A8.html" class="">Proxy代理</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/3%E3%80%81vm2%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83.html" class="">3、vm2运行环境</a>
</li>

<li class="divider"></li>



<li><a href="http://www.mkdocs.org">
Published with MkDocs
</a></li>

<li><a href="https://github.com/GitbookIO/theme-default">
Theme by GitBook
</a></li>
</ul>

</nav>

</div> <!-- end of book-summary -->

<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">

<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="." ></a>
</h1>

</div> <!-- end of book-header -->

<div class="page-wrapper" tabindex="-1" role="main">
<div class="page-inner">
<div id="book-search-results">
<div class="search-noresults">

<section class="normal markdown-section">



<h1 id="scrapy">十六、Scrapy中间件</h1>
<h5 id="_1">学习目标：</h5>
<ol>
<li>应用 scrapy中使用中间件使用随机UA的方法</li>
<li>了解 scrapy中使用代理ip的的方法</li>
</ol>
<hr />
<h3 id="1scrapy">1、scrapy中间件的分类和作用</h3>
<h5 id="11-scrapy">1.1 scrapy中间件的分类</h5>
<p>根据scrapy运行流程中所在位置不同分为：</p>
<ol>
<li>下载中间件</li>
<li>爬虫中间件</li>
</ol>
<h5 id="12-scrapy">1.2 scrapy中间的作用</h5>
<ol>
<li>主要功能是在爬虫运行过程中进行一些处理，如对非200响应的重试（重新构造Request对象yield给引擎）</li>
<li>也可以对header以及cookie进行更换和处理</li>
<li>其他根据业务需求实现响应的功能</li>
</ol>
<p>但在scrapy默认的情况下 两种中间件都在middlewares.py一个文件中</p>
<p>爬虫中间件使用方法和下载中间件相同，常用下载中间件</p>
<h3 id="2">2、下载中间件的使用方法：</h3>
<blockquote>
<p>接下来我们对爬虫进行修改完善，通过下载中间件来学习如何使用中间件 编写一个Downloader Middlewares和我们编写一个pipeline一样，定义一个类，然后在setting中开启</p>
</blockquote>
<p>Downloader Middlewares默认的方法：在中间件类中，有时需要重写处理请求或者响应的方法】</p>
<ul>
<li>process_request(self, request, spider)：【此方法是用的最多的】</li>
<li>当每个request通过下载中间件时，该方法被调用。</li>
<li>返回None值：继续请求  没有return也是返回None，该request对象传递给下载器，或通过引擎传递给其他权重低的process_request方法 【如果所有的下载器中间件都返回为None，则请求最终被交给下载器处理】</li>
<li>返回Response对象：不再请求，把response返回给引擎【如果返回为请求，则将请求交给调度器】</li>
<li>
<p>返回Request对象：把request对象交给调度器进行后续的请求  </p>
</li>
<li>
<p>process_response(self, request, response, spider)：</p>
</li>
<li>当下载器完成http请求，传递响应给引擎的时候调用</li>
<li>返回Resposne：通过引擎交给爬虫处理或交给权重更低的其他下载中间件的process_response方法 【如果返回为请求，则将请求交给调度器】</li>
<li>
<p>返回Request对象：交给调取器继续请求，此时将不通过其他权重低的process_request方法  【将响应对象交给spider进行解析】</p>
</li>
<li>
<p>process_exception(self, request, exception, spider):</p>
</li>
<li>
<p>请求出现异常的时候进行diaoy</p>
</li>
<li>
<p>比如当前请求被识别为爬虫 可以使用代理</p>
<p><code>python
def process_exception(self, request, exception, spider):
    request.meta['proxy'] = 'ip地址'
  request.dont_filter = True  # 因为默认请求是去除重复的，因为当前已经请求过，所以需要设置当前为不去重
    return request  # 将修正后的对象重新进行请求</code></p>
</li>
<li>
<p>在settings.py中配置开启中间件，权重值越小越优先执行  【同管道的注册使用】</p>
</li>
<li>
<p>spider参数：为爬虫中类的实例化可以在这里进行调用爬虫中的属性</p>
</li>
</ul>
<p>如：spider.name</p>
<h4 id="21">2.1 当前中间件的简单使用</h4>
<p>spiders.py（爬虫代码）</p>
<pre><code class="language-python">class WySpider(scrapy.Spider):
    name = 'wy'
    start_urls = ['http://www.baidu123.com/']  # 给一个正确的网址
    # start_urls = ['http://www.baidu123.com/']  # 给一个错误的网址
    def parse(self, response, **kwargs):
        pass
</code></pre>
<p>settings.py(开启中间件)</p>
<pre><code class="language-python">DOWNLOADER_MIDDLEWARES = {
   'wangyi.middlewares.WangyiDownloaderMiddleware': 543,
}
</code></pre>
<p>middlewares.py</p>
<pre><code class="language-python">class WangyiDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        print('process_request')
        return None

    def process_response(self, request, response, spider):
        print('process_response')
        return response

    def process_exception(self, request, exception, spider):
        print('process_exception')
        return request
</code></pre>
<p>运行查看</p>
<p>运行查看</p>
<p><img alt="image-20220926140805300" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20220926140805300.png" /></p>
<p>修改spiders.py  给一个错误的网址在进行查看，会发现当前会循环执行</p>
<p><img alt="image-20220926141048451" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20220926141048452.png" /></p>
<h3 id="3user-agent">3、定义实现随机User-Agent的下载中间件</h3>
<h5 id="31-settingsua">3.1 在settings中添加UA的列表</h5>
<pre><code class="language-python">USER_AGENTS_LIST = [ 
&quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)&quot;, \
&quot;Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)&quot;, \
&quot;Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)&quot;, \
&quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)&quot;, \
&quot;Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6&quot;, \
&quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1&quot;, \
&quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0&quot;, \
&quot;Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5&quot; ]
</code></pre>
<h5 id="32-middlewarespy">3.2 在middlewares.py中完善代码</h5>
<pre><code class="language-python">import random
from Tencent.settings import USER_AGENTS_LIST # 注意导入路径,请忽视pycharm的错误提示

class UserAgentMiddleware(object):
    def process_request(self, request, spider):
        user_agent = random.choice(USER_AGENTS_LIST)
        request.headers['User-Agent'] = user_agent
</code></pre>
<h5 id="33-tencentpy">3.3 在爬虫文件tencent.py的每个解析函数中添加</h5>
<pre><code class="language-python">class CheckUA:
    def process_response(self,request,response,spider):
        print(request.headers['User-Agent'])
</code></pre>
<h5 id="34-settings">3.4 在settings中设置开启自定义的下载中间件，设置方法同管道</h5>
<pre><code class="language-python">DOWNLOADER_MIDDLEWARES = {
   'Tencent.middlewares.UserAgentMiddleware': 543,
}
</code></pre>
<h5 id="35">3.5 运行爬虫观察现象</h5>
<h3 id="4ip">4、代理ip的使用</h3>
<h5 id="41">4.1 思路分析</h5>
<ol>
<li>
<p>代理添加的位置：request.meta中增加<code>proxy</code>字段</p>
</li>
<li>
<p>获取一个代理ip，赋值给</p>
</li>
</ol>
<p><code>python
   request.meta['proxy']</code></p>
<ul>
<li>代理池中随机选择代理ip</li>
<li>代理ip的webapi发送请求获取一个代理ip</li>
</ul>
<h5 id="42">4.2 具体实现</h5>
<pre><code class="language-python">class ProxyMiddleware(object):
    def process_request(self,request,spider):
        proxy = random.choice(proxies) # proxies可以在settings.py中，也可以来源于代理ip的webapi
        # proxy = 'http://192.168.1.1:8118'
        request.meta['proxy'] = proxy
        return None # 可以不写return
</code></pre>
<h5 id="43-ip">4.3 检测代理ip是否可用</h5>
<p>在使用了代理ip的情况下可以在下载中间件的process_response()方法中处理代理ip的使用情况，如果该代理ip不能使用可以替换其他代理ip</p>
<pre><code class="language-python">class ProxyMiddleware(object):
    def process_response(self, request, response, spider):
        if response.status != '200' and response.status != '302':
            #此时对代理ip进行操作，比如删除
            return request
</code></pre>
<h5 id="44">4.4 快代理的购买与使用</h5>
<p>网址：https://www.kuaidaili.com/</p>
<ul>
<li>输入网址   点击购买代理</li>
</ul>
<p><img alt="image-20221222111151687" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20221222111151687.png" /></p>
<ul>
<li>选择你想购买代理的类型</li>
</ul>
<p><img alt="image-20221222111226189" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20221222111226189.png" /></p>
<ul>
<li>以隧道代理为例  点击购买</li>
</ul>
<p><img alt="image-20221222111311561" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20221222111311561.png" /></p>
<ul>
<li>购买后点击 文档中心</li>
</ul>
<p><img alt="image-20221222111528183" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20221222111528183.png" /></p>
<ul>
<li>点击</li>
</ul>
<p><img alt="image-20221222111550153" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20221222111550153.png" /></p>
<ul>
<li>选择隧道代理</li>
</ul>
<p><img alt="image-20221222111626751" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20221222111626751.png" /></p>
<ul>
<li>向下拉 选择你当前要使用代理的模块</li>
</ul>
<p>我们是scrapy使用隧道 所以选择   <strong>Python-Scrapy</strong></p>
<p><img alt="image-20221222111731483" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20221222111731483.png" /></p>
<ul>
<li>找到<strong>middlewares.py</strong></li>
</ul>
<p><img alt="image-20221222111824669" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20221222111824669.png" /></p>
<ul>
<li>将中间件类代码复制到当前自己scrapy的中间件得文件中即可</li>
</ul>
<p><img alt="image-20221222112013039" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20221222112013039.png" /></p>
<ul>
<li>按照步骤开启中间件与填写自己的用户名与密码等信息即可</li>
</ul>
<h3 id="_2">总结</h3>
<p>中间件的使用：</p>
<ol>
<li>完善中间件代码：</li>
<li>process_request(self, request, spider)：<ul>
<li>当每个request通过下载中间件时，该方法被调用。</li>
<li>返回None值：继续请求</li>
<li>返回Response对象：不再请求，把response返回给引擎</li>
<li>返回Request对象：把request对象交给调度器进行后续的请求</li>
</ul>
</li>
<li>process_response(self, request, response, spider)：<ul>
<li>当下载器完成http请求，传递响应给引擎的时候调用</li>
<li>返回Resposne：交给process_response来处理</li>
<li>返回Request对象：交给调取器继续请求</li>
</ul>
</li>
<li>需要在settings.py中开启中间件 DOWNLOADER_MIDDLEWARES = { 'myspider.middlewares.UserAgentMiddleware': 543, }</li>
</ol>
<h3 id="5">5、爬取网易新闻</h3>
<p>url：https://news.163.com/</p>
<h4 id="51">5.1 爬取前准备</h4>
<ul>
<li>scrapy startproject wangyi</li>
<li>cd wangyi</li>
<li>scrapy genspider wy  https://news.163.com/</li>
</ul>
<h4 id="52">5.2 爬取前分析</h4>
<p>抓取 国内 国际 军事 航空</p>
<ul>
<li>分析 </li>
</ul>
<p>国内等数据是由动态加载的 并不是跟着当前的请求一起返回的 </p>
<p><img alt="image-20220509095648595" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20220509095648595.png" /></p>
<p>解决方式2种</p>
<ol>
<li>
<p>通过selenium配合爬虫抓取页面进行数据</p>
</li>
<li>
<p>找到加载动态数据的url地址  通过爬虫进行抓取</p>
<p><img alt="image-20220509100159399" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20220509100159399.png" /></p>
<p><img alt="image-20220509100235422" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20220509100235422.png" /></p>
</li>
</ol>
<p><img alt="image-20220509100359070" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20220509100359070.png" /></p>
<p>将找到的URL放到浏览器中进行请求  效果如下</p>
<p><img alt="image-20220509100447858" src="imgs/16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.assets/image-20220509100447858.png" /></p>
<h4 id="53">5.3 代码配置</h4>
<ul>
<li>配置文件处理settings.py</li>
</ul>
<p>```python
  # Scrapy settings for wangyi project
  BOT_NAME = 'wangyi'</p>
<p>SPIDER_MODULES = ['wangyi.spiders']
  NEWSPIDER_MODULE = 'wangyi.spiders'</p>
<p># 默认请求头
  USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'</p>
<p># 用于更换随机请求头
  USER_AGENTS_LIST = [
  "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
  "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
  "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)",
  "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)",
  "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
  "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
  "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0",
  "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5" ]</p>
<p>LOG_LEVEL = 'ERROR'
  ROBOTSTXT_OBEY = False</p>
<p>DOWNLOADER_MIDDLEWARES = {
     'wangyi.middlewares.WangyiDownloaderMiddleware': 543,
  }
  ITEM_PIPELINES = {
     'wangyi.pipelines.WangyiPipeline': 300,
  }
  ```</p>
<ul>
<li>爬虫代码wy.py</li>
</ul>
<p>```python
  import scrapy
  from selenium import webdriver
  from selenium.webdriver import ChromeOptions
  from selenium.webdriver.chrome.options import Options
  from wangyi.items import WangyiItem</p>
<p>class WySpider(scrapy.Spider):
      name = 'wy'
      # allowed_domains = ['news.163.com']
      start_urls = ['https://news.163.com/domestic/']
      # li_index = [1, 2, 4, 5]  # 当前要爬取菜单的索引位置
      li_index = [1, 2]  # 当前要爬取菜单的索引位置
      page_url = []
      # 隐藏浏览器界面
      chrome_option = Options()
      chrome_option.add_argument('--headless')
      chrome_option.add_argument('--disable-gpu')
      # 防止检测
      option = ChromeOptions()
      option.add_experimental_option('excludeSwitches', ['enable-automation'])
      # 导入配置
      driver = webdriver.Chrome(chrome_options=chrome_option, options=option)
      def parse(self, response, **kwargs):
          # 抓取 国内 国际 军事 航空
          menu = response.xpath('/html/body/div/div[3]/div[2]/div[2]/div/ul/li/a/@href').extract()
          # 循环获取当前我们要抓取栏目的url
          for i in range(len(menu)):
              if i in self.li_index:
                  url = menu[i]
                  self.page_url.append(url)
                  # 向详情页发起请求
                  yield scrapy.Request(url, callback=self.parse_detail)
      # 对栏目页进行请求
      def parse_detail(self, response):
          # 获取每个新闻的url
          detail_href_list = response.xpath('/html/body/div/div[3]/div[3]/div[1]/div[1]/div/ul/li/div/div/a/@href').extract()
          # print(detail_href_list)
          for url in detail_href_list:
              print(url)
              yield scrapy.Request(url, callback=self.parse_detail_con)</p>
<pre><code>  # 对于新闻详情页进行解析
  def parse_detail_con(self, response):
      # 实例化item
      item = WangyiItem()
      # 匹配标题
      title = response.xpath('//*[@id="container"]/div[1]/h1/text()').extract_first()
      # 匹配内容
      con = ''.join(response.xpath('//*[@id="content"]/div[2]//text()').extract())
      item['title'] = title
      item['con'] = con
      print(item)
      yield item
</code></pre>
<p>```</p>
<ul>
<li>Items.py</li>
</ul>
<p><code>python
  import scrapy
  class WangyiItem(scrapy.Item):
      # define the fields for your item here like:
      # name = scrapy.Field()
      title = scrapy.Field()
      content = scrapy.Field()</code></p>
<ul>
<li>Middlewares.py</li>
</ul>
<p>```python
  from wangyi.settings import USER_AGENTS_LIST
  import random
  from scrapy.http import HtmlResponse</p>
<p>class WangyiDownloaderMiddleware:</p>
<pre><code>  @classmethod
  def from_crawler(cls, crawler):
      # This method is used by Scrapy to create your spiders.
      s = cls()
      crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
      return s

  def process_request(self, request, spider):
      request.headers['User-Agent'] = random.choice(USER_AGENTS_LIST)
      # print(request.headers)
      return None

  def process_response(self, request, response, spider):
      # Called with the response returned from the downloader.
      driver = spider.driver  # 获取到selenium
      if request.url in spider.page_url:
          driver.get(request.url)
          # 滚动条滚动到最下方
          driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')
          time.sleep(1)
          # 拖动两次
          driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')
          time.sleep(1)
          text = driver.page_source
          # 篡改响应对象并返回
          return HtmlResponse(url=request.url, body=text, encoding='UTF-8', request=request)
      return response

  def process_exception(self, request, exception, spider):
      # Called when a download handler or a process_request()
      pass
  def spider_opened(self, spider):
      spider.logger.info('Spider opened: %s' % spider.name)
</code></pre>
<p>```</p>


</section>
</div> <!-- end of search-noresults -->
<div class="search-results">
<div class="has-results">

<h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
<ul class="search-results-list"></ul>

</div> <!-- end of has-results -->
<div class="no-results">

<h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>

</div> <!-- end of no-results -->
</div> <!-- end of search-results -->
</div> <!-- end of book-search-results -->

</div> <!-- end of page-inner -->
</div> <!-- end of page-wrapper -->

</div> <!-- end of body-inner -->

</div> <!-- end of book-body -->
<script src="./js/main.js"></script>
<script src="search/main.js"></script>
<script src="./js/gitbook.min.js"></script>
<script src="./js/theme.min.js"></script>
</body>
</html>