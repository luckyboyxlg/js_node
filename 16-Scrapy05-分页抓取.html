<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<title>十六、scrapy的crawlspider爬虫 - js node</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="generator" content="mkdocs-1.6.1, mkdocs-gitbook-1.0.7">

<link rel="shortcut icon" href="./images/favicon.ico" type="image/x-icon">
<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta rel="next" href="" />
<link href="./css/style.min.css" rel="stylesheet"> 
</head>

<body>
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input type="text" placeholder="Type to search" />
</div> <!-- end of book-search-input -->

<nav role="navigation">
<ul class="summary">
<li>
<a href="index.html" target="_blank" class="custom-link">js node</a>
</li>
<li class="divider"></li>
<li class="chapter" data-path="index.html">
<a href="index.html">Welcome to MkDocs</a>
<li class="chapter" data-path="01%E3%80%81html.html">
<a href="01%E3%80%81html.html">一、HTML</a>
<li class="chapter" data-path="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">
<a href="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">二、CSS层叠样式表</a>
<li class="chapter" data-path="03%E3%80%81%E6%AD%A3%E5%88%99.html">
<a href="03%E3%80%81%E6%AD%A3%E5%88%99.html">三、正则</a>
<li class="chapter" data-path="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">
<a href="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">四、beautifulsoup</a>
<li class="chapter" data-path="05%E3%80%81xpath.html">
<a href="05%E3%80%81xpath.html">五、xpath</a>
<li class="chapter" data-path="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">
<a href="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">六、爬虫入门</a>
<li class="chapter" data-path="07%E3%80%81urllib%E4%B8%8Erequests.html">
<a href="07%E3%80%81urllib%E4%B8%8Erequests.html">七、urllib与requests</a>
<li class="chapter" data-path="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">
<a href="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">八、多进程</a>
<li class="chapter" data-path="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">
<a href="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">九、线程</a>
<li class="chapter" data-path="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">
<a href="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">十、携程</a>
<li class="chapter" data-path="11%E3%80%81selenium.html">
<a href="11%E3%80%81selenium.html">十一、selenium</a>
<li class="chapter" data-path="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">
<a href="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">十二、MySQL数据库</a>
<li class="chapter" data-path="13%E3%80%81Mongodb.html">
<a href="13%E3%80%81Mongodb.html">十三、Mongodb</a>
<li class="chapter" data-path="14%E3%80%81redis.html">
<a href="14%E3%80%81redis.html">十四、Redis数据库</a>
<li class="chapter" data-path="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">
<a href="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">十五、面向对象</a>
<li class="chapter" data-path="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">
<a href="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">十六、Scrapy框架初认识</a>
<li class="chapter" data-path="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">
<a href="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">十六、Scrapy深入使用-存储</a>
<li class="chapter" data-path="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">
<a href="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">十六、scrapy模拟登陆&amp;分页</a>
<li class="chapter" data-path="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">
<a href="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">十六、Scrapy中间件</a>
<li class="chapter active" data-path="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">
<a href="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">十六、scrapy的crawlspider爬虫</a>
<li class="chapter" data-path="16-Scrapy06-scrapy_redis.html">
<a href="16-Scrapy06-scrapy_redis.html">十六、scrapy_redis</a>
<li class="chapter" data-path="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">
<a href="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">十七、js数据类型</a>
<li class="chapter" data-path="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">
<a href="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">十八、js运算符与流程控制</a>
<li class="chapter" data-path="19%E3%80%81js%E6%95%B0%E7%BB%84.html">
<a href="19%E3%80%81js%E6%95%B0%E7%BB%84.html">十九、js数组</a>
<li class="chapter" data-path="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">
<a href="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">二十、js字符串</a>
<li class="chapter" data-path="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">
<a href="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">二十一、js对象与时间</a>
<li class="chapter" data-path="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">
<a href="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">二十二、js函数</a>
<li class="chapter" data-path="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">
<a href="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">二十三、Javascript进阶</a>
<li class="chapter" data-path="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">
<a href="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">二十四、浏览器对象模型BOM</a>
<li class="chapter" data-path="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">
<a href="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">二十五、DOM操作</a>
<li class="chapter" data-path="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">
<a href="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">二十六、jQuery</a>
<li class="chapter" data-path="27%E3%80%81%E9%80%86%E5%90%9101.html">
<a href="27%E3%80%81%E9%80%86%E5%90%9101.html">二十七、JS逆向01</a>
<li class="chapter" data-path="28%E3%80%81%E9%80%86%E5%90%9102.html">
<a href="28%E3%80%81%E9%80%86%E5%90%9102.html">二十八、逆向02</a>
<li class="chapter" data-path="29%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">
<a href="29%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">二十九、微信小程序逆向开发</a>
<li class="chapter" data-path="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">
<a href="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">三十、网易易盾</a>
<li class="chapter" data-path="31%E3%80%81rpc.html">
<a href="31%E3%80%81rpc.html">三十一、RPC</a>
<li class="chapter" data-path="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">
<a href="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">三十二、TLS指纹和JA3指纹绕过</a>
<li class="chapter" data-path="33%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">
<a href="33%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">三十三、高级逆向</a>
<li class="chapter" data-path="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">
<a href="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">三十四、高级逆向</a>
<li class="chapter" data-path="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">
<a href="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">逆向实战案例笔记</a>
<li class="header">Pyexecjs与npm配置</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/execjs%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8.html" class="">Execjs介绍以及安装和使用</a>
</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/%E4%B8%80%E6%AC%A1%E6%80%A7%E8%A7%A3%E5%86%B3%E6%8E%89npm%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98.html" class="">更换npm为国内镜像</a>
</li>

<li class="header">补环境</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/1%E3%80%81%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%8E%E4%BA%8B%E4%BB%B6.html" class="">补环境</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/2%E3%80%81Proxy%E4%BB%A3%E7%90%86%E5%99%A8.html" class="">Proxy代理</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/3%E3%80%81vm2%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83.html" class="">3、vm2运行环境</a>
</li>

<li class="divider"></li>



<li><a href="http://www.mkdocs.org">
Published with MkDocs
</a></li>

<li><a href="https://github.com/GitbookIO/theme-default">
Theme by GitBook
</a></li>
</ul>

</nav>

</div> <!-- end of book-summary -->

<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">

<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="." ></a>
</h1>

</div> <!-- end of book-header -->

<div class="page-wrapper" tabindex="-1" role="main">
<div class="page-inner">
<div id="book-search-results">
<div class="search-noresults">

<section class="normal markdown-section">



<h1 id="scrapycrawlspider">十六、scrapy的crawlspider爬虫</h1>
<h5 id="_1">学习目标：</h5>
<ol>
<li>了解 crawlspider的作用</li>
<li>应用 crawlspider爬虫创建的方法</li>
<li>应用 crawlspider中rules的使用 </li>
</ol>
<hr />
<h3 id="1crawlspider">1、crawlspider是什么</h3>
<blockquote>
<p>回顾之前的代码中，我们有很大一部分时间在寻找下一页的url地址或者是内容的url地址上面，这个过程能更简单一些么？</p>
</blockquote>
<h5 id="_2">思路：</h5>
<ol>
<li>从response中提取所有的满足规则的url地址</li>
<li>自动的构造自己requests请求，发送给引擎</li>
</ol>
<p>对应的<strong>crawlspider就可以实现上述需求，能够匹配满足条件的url地址，组装成Reuqest对象后自动发送给引擎，同时能够指定callback函数</strong></p>
<p><strong>即：crawlspider爬虫可以按照规则自动获取连接</strong></p>
<h3 id="2crawlspidertop250">2、crawlspider豆瓣TOP250爬虫</h3>
<blockquote>
<p>通过crawlspider爬取豆瓣TOP250详情页的信息</p>
<p>url：https://movie.douban.com/top250</p>
</blockquote>
<h5 id="_3">思路分析：</h5>
<ol>
<li>定义一个规则，来进行列表页翻页，follow需要设置为True</li>
<li>定义一个规则，实现从列表页进入详情页，并且指定回调函数</li>
<li>在详情页提取数据</li>
</ol>
<h5 id="linkextractorallowhref">注意：连接提取器LinkExtractor中的allow对应的正则表达式匹配的是href属性的值</h5>
<h3 id="3crawlspider">3、创建crawlspider爬虫并观察爬虫内的默认内容</h3>
<h5 id="31-crawlspider">3.1 创建crawlspider爬虫：</h5>
<p>scrapy startproject project</p>
<p>cd project</p>
<p>scrapy genspider -t crawl douban book.douban.com/latest?subcat=%E5%85%A8%E9%83%A8&amp;p=1</p>
<p>url: 豆瓣图书 https://book.douban.com/latest?subcat=%E5%85%A8%E9%83%A8&amp;p=1</p>
<h5 id="32-spider">3.2 spider中默认生成的内容如下：</h5>
<pre><code class="language-python">import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class BookSpider(CrawlSpider):
    name = 'douban'
    # allowed_domains = ['https://movie.douban.com/top250']

    start_urls = ['https://movie.douban.com/top250']
    # 匹配页码地址
    link = LinkExtractor(allow=r'start=\d+&amp;filter=')
    # 匹配详情页地址
    link_detail = LinkExtractor(allow=r'https://movie.douban.com/subject/\d+/')
    # allow值什么都不写 则为提取所有的url
    link_all = LinkExtractor(allow=r'')
    rules = (
        # Rule(link, callback='parse_item', follow=True),
        # Rule(link_detail, callback='parse_detail_item', follow=False),
        Rule(link_all, callback='parse_all_item', follow=True),
    )

    def parse_item(self, response):
        item = {}
        print(response)
        return item

    def parse_detail_item(self, response):
        item = {}
        print(response)
        return item

    def parse_all_item(self, response):
        item = {}
        print(response)
        return item
</code></pre>
<h5 id="33-scrapyspider">3.3 观察跟普通的scrapy.spider的区别</h5>
<p>在crawlspider爬虫中，没有parse函数</p>
<h5 id="rules">重点在rules中：</h5>
<ol>
<li>rules是一个元组或者是列表，包含的是Rule对象</li>
<li>Rule表示规则，其中包含LinkExtractor,callback和follow等参数</li>
<li>LinkExtractor:连接提取器，可以通过正则或者是xpath来进行url地址的匹配</li>
<li>callback :表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数的处理</li>
<li>follow：连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，True表示会，Flase表示不会</li>
</ol>
<h3 id="4crawlspider">4、crawlspider使用的注意点：</h3>
<ol>
<li>除了用命令<code>scrapy genspider -t crawl &lt;爬虫名&gt; &lt;allowed_domail&gt;</code>创建一个crawlspider的模板，页可以手动创建</li>
<li>crawlspider中不能再有以parse为名的数据提取方法，该方法被crawlspider用来实现基础url提取等功能</li>
<li>Rule对象中LinkExtractor为固定参数，其他callback、follow为可选参数</li>
<li>不指定callback且follow为True的情况下，满足rules中规则的url还会被继续提取和请求</li>
<li>如果一个被提取的url满足多个Rule，那么会从rules中选择一个满足匹配条件的Rule执行</li>
</ol>
<h3 id="5crawlspider">5、了解crawlspider其他知识点</h3>
<ul>
<li>
<p>链接提取器LinkExtractor的更多常见参数</p>
</li>
<li>
<p>allow: 满足括号中的're'表达式的url会被提取，如果为空，则全部匹配</p>
</li>
<li>
<p>deny: 满足括号中的're'表达式的url不会被提取，优先级高于allow</p>
</li>
<li>
<p>allow_domains: 会被提取的链接的domains(url范围)，如：<code>['https://movie.douban.com/top250']</code></p>
</li>
<li>
<p>deny_domains: 不会被提取的链接的domains(url范围)</p>
</li>
<li>
<p><strong>restrict_xpaths: 使用xpath规则进行匹配，和allow共同过滤url，即xpath满足的范围内的url地址会被提取</strong></p>
<p>如：<code>restrict_xpaths='//div[@class="pagenav"]'</code></p>
</li>
<li>
<p>restrict_css: 接收一堆css选择器, 可以提取符合要求的css选择器的链接</p>
</li>
<li>
<p>attrs: 接收一堆属性名, 从某个属性中提取链接, 默认href</p>
</li>
<li>
<p>tags: 接收一堆标签名, 从某个标签中提取链接, 默认a, area</p>
</li>
</ul>
<p>值得注意的, <strong>在提取到的url中, 是有重复的内容的. 但是我们不用管. scrapy会自动帮我们过滤掉重复的url请求</strong></p>
<ul>
<li>模拟使用</li>
</ul>
<p>正则用法：　links1 = LinkExtractor(allow=r'list_23_\d+.html')</p>
<p>xpath用法： links2 = LinkExtractor(restrict_xpaths=r'//div[@class="x"]')</p>
<p>css用法：  links3 = LinkExtractor(restrict_css='.x')</p>
<p>5.提取连接</p>
<ul>
<li>
<p>Rule常见参数</p>
</li>
<li>
<p>LinkExtractor: 链接提取器，可以通过正则或者是xpath来进行url地址的匹配</p>
</li>
<li>callback: 表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数的处理</li>
<li>follow: 连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，默认True表示会，Flase表示不会</li>
<li>process_links: 当链接提取器LinkExtractor获取到链接列表的时候调用该参数指定的方法，这个自定义方法可以用来过滤url，且这个方法执行后才会执行callback指定的方法</li>
</ul>
<h3 id="_4">总结</h3>
<ol>
<li>crawlspider的作用：crawlspider可以按照规则自动获取连接</li>
<li>crawlspider爬虫的创建：scrapy genspider -t crawl xxx  www.xxx.com</li>
<li>crawlspider中rules的使用：</li>
<li>rules是一个元组或者是列表，包含的是Rule对象</li>
<li>Rule表示规则，其中包含LinkExtractor,callback和follow等参数</li>
<li>LinkExtractor:连接提取器，可以通过正则或者是xpath来进行url地址的匹配</li>
<li>callback :表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数的处理</li>
<li>follow：连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，True表示会，Flase表示不会</li>
</ol>
<h4 id="_5"></h4>


</section>
</div> <!-- end of search-noresults -->
<div class="search-results">
<div class="has-results">

<h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
<ul class="search-results-list"></ul>

</div> <!-- end of has-results -->
<div class="no-results">

<h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>

</div> <!-- end of no-results -->
</div> <!-- end of search-results -->
</div> <!-- end of book-search-results -->

</div> <!-- end of page-inner -->
</div> <!-- end of page-wrapper -->

</div> <!-- end of body-inner -->

</div> <!-- end of book-body -->
<script src="./js/main.js"></script>
<script src="search/main.js"></script>
<script src="./js/gitbook.min.js"></script>
<script src="./js/theme.min.js"></script>
</body>
</html>