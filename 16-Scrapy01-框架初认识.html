<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<title>十六、Scrapy框架初认识 - js node</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="generator" content="mkdocs-1.6.1, mkdocs-gitbook-1.0.7">

<link rel="shortcut icon" href="./images/favicon.ico" type="image/x-icon">
<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta rel="next" href="" />
<link href="./css/style.min.css" rel="stylesheet"> 
</head>

<body>
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input type="text" placeholder="Type to search" />
</div> <!-- end of book-search-input -->

<nav role="navigation">
<ul class="summary">
<li>
<a href="index.html" target="_blank" class="custom-link">js node</a>
</li>
<li class="divider"></li>
<li class="chapter" data-path="index.html">
<a href="index.html">Welcome to MkDocs</a>
<li class="chapter" data-path="01%E3%80%81html.html">
<a href="01%E3%80%81html.html">一、HTML</a>
<li class="chapter" data-path="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">
<a href="02%E3%80%81CSS%E5%B1%82%E5%8F%A0%E6%A0%B7%E5%BC%8F%E8%A1%A8.html">二、CSS层叠样式表</a>
<li class="chapter" data-path="03%E3%80%81%E6%AD%A3%E5%88%99.html">
<a href="03%E3%80%81%E6%AD%A3%E5%88%99.html">三、正则</a>
<li class="chapter" data-path="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">
<a href="04%E3%80%81BS4%E8%A7%A3%E6%9E%90%E5%AE%8C%E6%95%B4.html">四、beautifulsoup</a>
<li class="chapter" data-path="05%E3%80%81xpath.html">
<a href="05%E3%80%81xpath.html">五、xpath</a>
<li class="chapter" data-path="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">
<a href="06%E3%80%81%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B8%8Eurllib%26requests.html">六、爬虫入门</a>
<li class="chapter" data-path="07%E3%80%81urllib%E4%B8%8Erequests.html">
<a href="07%E3%80%81urllib%E4%B8%8Erequests.html">七、urllib与requests</a>
<li class="chapter" data-path="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">
<a href="08%E3%80%81%E5%A4%9A%E8%BF%9B%E7%A8%8B.html">八、多进程</a>
<li class="chapter" data-path="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">
<a href="09%E3%80%81%E7%BA%BF%E7%A8%8B.html">九、线程</a>
<li class="chapter" data-path="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">
<a href="10%E3%80%81%E5%8D%8F%E7%A8%8B.html">十、携程</a>
<li class="chapter" data-path="11%E3%80%81selenium.html">
<a href="11%E3%80%81selenium.html">十一、selenium</a>
<li class="chapter" data-path="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">
<a href="12%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.html">十二、MySQL数据库</a>
<li class="chapter" data-path="13%E3%80%81Mongodb.html">
<a href="13%E3%80%81Mongodb.html">十三、Mongodb</a>
<li class="chapter" data-path="14%E3%80%81redis.html">
<a href="14%E3%80%81redis.html">十四、Redis数据库</a>
<li class="chapter" data-path="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">
<a href="15%E3%80%81%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1.html">十五、面向对象</a>
<li class="chapter active" data-path="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">
<a href="16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.html">十六、Scrapy框架初认识</a>
<li class="chapter" data-path="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">
<a href="16-Scrapy02%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8-%E5%AD%98%E5%82%A8.html">十六、Scrapy深入使用-存储</a>
<li class="chapter" data-path="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">
<a href="16-Scrapy03-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E4%BB%A5%E5%8F%8A%E5%88%86%E9%A1%B5.html">十六、scrapy模拟登陆&amp;分页</a>
<li class="chapter" data-path="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">
<a href="16-Scrapy04-%E4%B8%AD%E9%97%B4%E4%BB%B6.html">十六、Scrapy中间件</a>
<li class="chapter" data-path="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">
<a href="16-Scrapy05-%E5%88%86%E9%A1%B5%E6%8A%93%E5%8F%96.html">十六、scrapy的crawlspider爬虫</a>
<li class="chapter" data-path="16-Scrapy06-scrapy_redis.html">
<a href="16-Scrapy06-scrapy_redis.html">十六、scrapy_redis</a>
<li class="chapter" data-path="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">
<a href="17%E3%80%81js%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.html">十七、js数据类型</a>
<li class="chapter" data-path="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">
<a href="18%E3%80%81%E8%BF%90%E7%AE%97%E7%AC%A6%E4%B8%8E%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6.html">十八、js运算符与流程控制</a>
<li class="chapter" data-path="19%E3%80%81js%E6%95%B0%E7%BB%84.html">
<a href="19%E3%80%81js%E6%95%B0%E7%BB%84.html">十九、js数组</a>
<li class="chapter" data-path="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">
<a href="20%E3%80%81js%E5%AD%97%E7%AC%A6%E4%B8%B2.html">二十、js字符串</a>
<li class="chapter" data-path="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">
<a href="21%E3%80%81js%E5%AF%B9%E8%B1%A1%E5%92%8C%E6%97%B6%E9%97%B4.html">二十一、js对象与时间</a>
<li class="chapter" data-path="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">
<a href="22%E3%80%81%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89.html">二十二、js函数</a>
<li class="chapter" data-path="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">
<a href="23%E3%80%81js%E8%BF%9B%E9%98%B6.html">二十三、Javascript进阶</a>
<li class="chapter" data-path="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">
<a href="24%E3%80%81BOM%E6%93%8D%E4%BD%9C.html">二十四、浏览器对象模型BOM</a>
<li class="chapter" data-path="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">
<a href="25%E3%80%81DOM%E6%93%8D%E4%BD%9C.html">二十五、DOM操作</a>
<li class="chapter" data-path="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">
<a href="26%E3%80%81jQuery%E6%93%8D%E4%BD%9C.html">二十六、jQuery</a>
<li class="chapter" data-path="27%E3%80%81%E9%80%86%E5%90%9101.html">
<a href="27%E3%80%81%E9%80%86%E5%90%9101.html">二十七、JS逆向01</a>
<li class="chapter" data-path="28%E3%80%81%E9%80%86%E5%90%9102.html">
<a href="28%E3%80%81%E9%80%86%E5%90%9102.html">二十八、逆向02</a>
<li class="chapter" data-path="29%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">
<a href="29%E3%80%81%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F.html">二十九、微信小程序逆向开发</a>
<li class="chapter" data-path="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">
<a href="30%E3%80%81%E7%BD%91%E6%98%93%E6%BB%91%E5%9D%97.html">三十、网易易盾</a>
<li class="chapter" data-path="31%E3%80%81rpc.html">
<a href="31%E3%80%81rpc.html">三十一、RPC</a>
<li class="chapter" data-path="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">
<a href="32%E3%80%81TLS%E6%8C%87%E7%BA%B9%E7%BB%95%E8%BF%87.html">三十二、TLS指纹和JA3指纹绕过</a>
<li class="chapter" data-path="33%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">
<a href="33%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">三十三、高级逆向</a>
<li class="chapter" data-path="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">
<a href="34%E3%80%81%E9%AB%98%E7%BA%A7%E9%80%86%E5%90%91.html">三十四、高级逆向</a>
<li class="chapter" data-path="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">
<a href="%E9%80%86%E5%90%91%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E7%AC%94%E8%AE%B0.html">逆向实战案例笔记</a>
<li class="header">Pyexecjs与npm配置</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/execjs%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8.html" class="">Execjs介绍以及安装和使用</a>
</li>

<li>
<a href="pyexecjs%E4%B8%8Enpm%E9%85%8D%E7%BD%AE/%E4%B8%80%E6%AC%A1%E6%80%A7%E8%A7%A3%E5%86%B3%E6%8E%89npm%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98.html" class="">更换npm为国内镜像</a>
</li>

<li class="header">补环境</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/1%E3%80%81%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%8E%E4%BA%8B%E4%BB%B6.html" class="">补环境</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/2%E3%80%81Proxy%E4%BB%A3%E7%90%86%E5%99%A8.html" class="">Proxy代理</a>
</li>

<li>
<a href="%E8%A1%A5%E7%8E%AF%E5%A2%83/3%E3%80%81vm2%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83.html" class="">3、vm2运行环境</a>
</li>

<li class="divider"></li>



<li><a href="http://www.mkdocs.org">
Published with MkDocs
</a></li>

<li><a href="https://github.com/GitbookIO/theme-default">
Theme by GitBook
</a></li>
</ul>

</nav>

</div> <!-- end of book-summary -->

<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">

<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="." ></a>
</h1>

</div> <!-- end of book-header -->

<div class="page-wrapper" tabindex="-1" role="main">
<div class="page-inner">
<div id="book-search-results">
<div class="search-noresults">

<section class="normal markdown-section">



<h1 id="scrapy">十六、Scrapy框架初认识</h1>
<h2 id="_1">一、前言</h2>
<h3 id="1">1、介绍</h3>
<p>前面我们学习了基础的爬虫实现方法和selenium以及mongodb数据库，那么接下来会我们学习一个上场率非常高的爬虫框架：scrapy</p>
<h3 id="2">2、内容</h3>
<ul>
<li>scrapy的基础概念和工作流程</li>
<li>scrapy入门使用</li>
</ul>
<h2 id="scrapy_1">二、scrapy的概念和流程</h2>
<h5 id="_2">学习目标：</h5>
<ol>
<li>了解 scrapy的概念</li>
<li>掌握 scrapy框架的运行流程</li>
<li>掌握 scrapy框架的作用</li>
</ol>
<h3 id="1scrapy">1、为什么学习scrapy？</h3>
<ol>
<li>能够让开发过程方便、快速</li>
<li>scrapy框架能够让我们的爬虫效率更高</li>
</ol>
<h3 id="2scrapy">2、什么是scrapy？</h3>
<p>文档地址：https://docs.scrapy.org/en/latest/</p>
<p>Scrapy 使用了Twisted['twɪstɪd]异步网络框架，可以加快我们的下载速度。</p>
<p><strong>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架</strong>，我们只需要实现少量的代码，就能够快速的抓取。</p>
<h3 id="3">3、异步和非阻塞的区别</h3>
<p>前面我们说Twisted是一个异步的网络框架，经常我们也听到一个词语叫做非阻塞，那么他们有什么区别呢？</p>
<p><img alt="同步和异步" src="imgs/16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.assets/%E5%90%8C%E6%AD%A5%E5%92%8C%E5%BC%82%E6%AD%A5-2520822.png" /></p>
<p><strong>异步</strong>：调用在发出之后，这个调用就直接返回，不管有无结果；异步是过程。 <strong>非阻塞</strong>：关注的是程序在等待调用结果（消息，返回值）时的状态，指在不能立刻得到结果之前，该调用不会阻塞当前线程。</p>
<h3 id="4scrapy">4、scrapy的工作流程</h3>
<h5 id="41">4.1 回顾之前的爬虫流程</h5>
<p><img alt="爬虫流程-1" src="imgs/16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.assets/%E7%88%AC%E8%99%AB%E6%B5%81%E7%A8%8B-1-2520822.png" /></p>
<h5 id="42">4.2 上面的流程可以改写为</h5>
<p><img alt="爬虫流程-2" src="imgs/16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.assets/%E7%88%AC%E8%99%AB%E6%B5%81%E7%A8%8B-2-2520822.png" /></p>
<h5 id="43-scrapy">4.3 scrapy的流程</h5>
<p><img alt="爬虫流程-3" src="imgs/16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.assets/%E7%88%AC%E8%99%AB%E6%B5%81%E7%A8%8B-3-1047805-2520822.png" /></p>
<h5 id="_3">其流程可以描述如下：</h5>
<ol>
<li>调度器把requests--&gt;引擎--&gt;下载中间件---&gt;下载器</li>
<li>下载器发送请求，获取响应----&gt;下载中间件----&gt;引擎---&gt;爬虫中间件---&gt;爬虫</li>
<li>爬虫提取url地址，组装成request对象----&gt;爬虫中间件---&gt;引擎---&gt;调度器</li>
<li>爬虫提取数据---&gt;引擎---&gt;管道</li>
<li>管道进行数据的处理和保存</li>
</ol>
<h5 id="_4">注意：</h5>
<ul>
<li>图中绿色线条的表示数据的传递</li>
<li>注意图中中间件的位置，决定了其作用</li>
<li>注意其中引擎的位置，所有的模块之前相互独立，只和引擎进行交互</li>
</ul>
<h5 id="44-scrapy">4.4 scrapy中每个模块的具体作用</h5>
<p><img alt="scrapy组件" src="imgs/16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.assets/scrapy%E7%BB%84%E4%BB%B6-2520822.png" /></p>
<h3 id="_5">小结</h3>
<ol>
<li>scrapy的概念：Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架</li>
<li>scrapy框架的运行流程以及数据传递过程：</li>
<li>调度器把requests--&gt;引擎--&gt;下载中间件---&gt;下载器</li>
<li>下载器发送请求，获取响应----&gt;下载中间件----&gt;引擎---&gt;爬虫中间件---&gt;爬虫</li>
<li>爬虫提取url地址，组装成request对象----&gt;爬虫中间件---&gt;引擎---&gt;调度器</li>
<li>爬虫提取数据---&gt;引擎---&gt;管道</li>
<li>管道进行数据的处理和保存</li>
<li>scrapy框架的作用：通过少量代码实现快速抓取</li>
<li>掌握scrapy中每个模块的作用： 引擎(engine)：负责数据和信号在不腰痛模块间的传递 调度器(scheduler)：实现一个队列，存放引擎发过来的request请求对象 下载器(downloader)：发送引擎发过来的request请求，获取响应，并将响应交给引擎 爬虫(spider)：处理引擎发过来的response，提取数据，提取url，并交给引擎 管道(pipeline)：处理引擎传递过来的数据，比如存储 下载中间件(downloader middleware)：可以自定义的下载扩展，比如设置代理ip 爬虫中间件(spider middleware)：可以自定义request请求和进行response过滤</li>
<li>理解异步和非阻塞的区别：异步是过程，非阻塞是状态</li>
</ol>
<h2 id="scrapy_2">三、scrapy的入门使用</h2>
<h5 id="_6">学习目标：</h5>
<ol>
<li>掌握 scrapy的安装</li>
<li>应用 创建scrapy的项目</li>
<li>应用 创建scrapy爬虫</li>
<li>应用 运行scrapy爬虫</li>
<li>应用 解析并获取scrapy爬虫中的数据</li>
</ol>
<h3 id="1scrapy_1">1、scrapy项目实现流程</h3>
<ol>
<li>创建一个scrapy项目:scrapy startproject mySpider</li>
<li>生成一个爬虫:scrapy genspider myspider <a href="www.xxx.cn">www.xxx.cn</a></li>
<li>提取数据:完善spider，使用xpath等方法</li>
<li>保存数据:pipeline中保存数据</li>
</ol>
<h3 id="2_1">2、安装</h3>
<p>scrapy当前最新版本为 2.11.0</p>
<p>scrapy-redis当前最新版本为  0.7.3</p>
<p>安装scrapy命令：</p>
<pre><code> pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scrapy==2.11.0
 pip install scrapy==2.11.0
 pip install scrapy-redis==0.7.3
</code></pre>
<p>如果安装失败. 请先升级一下pip.  然后重新安装scrapy即可. </p>
<p>最新版本的pip升级完成后. 安装依然失败, 可以根据报错信息进行一点点的调整, 多试几次pip. 直至success. </p>
<p>注意：</p>
<p>如果上述过程还是无法正常安装scrapy, 可以考虑用下面的方案来安装:</p>
<p>如果上述过程还是无法正常安装scrapy, 可以考虑用下面的方案来安装:</p>
<ol>
<li>安装wheel</li>
</ol>
<p><code>pip install wheel</code></p>
<ol>
<li>下载twisted安装包, https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</li>
</ol>
<p><img alt="image-20210803144429440" src="imgs/16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.assets/image-20210803144429440-2520822.png" /></p>
<ol>
<li>用wheel安装twisted. </li>
</ol>
<p><code>pip install Twisted‑21.7.0‑py3‑none‑any.whl</code></p>
<ol>
<li>安装pywin32</li>
</ol>
<p><code>pip install pywin32</code></p>
<ol>
<li>安装scrapy</li>
</ol>
<p><code>pip install scrapy</code></p>
<p>在控制台输入<code>scrapy version</code>能显示版本号. 则安装成功</p>
<p><strong>注意：</strong></p>
<p>如果当前运行<code>scrapy version</code>  显示<code>不是内部或外部命令，也不是可运行的程序或批处理文件。</code></p>
<p>解决：</p>
<ol>
<li>升级pip 后重新安装</li>
<li>如果还不行则安装scrapy后配置环境变量</li>
</ol>
<p>参考</p>
<p>https://blog.csdn.net/hulabula/article/details/119725713</p>
<h3 id="3scrapy">3、创建scrapy项目</h3>
<p>创建scrapy项目的命令：scrapy startproject +&lt;项目名字&gt;</p>
<p>示例：scrapy startproject myspider</p>
<h4 id="_7">生成的目录和文件结果如下：</h4>
<p><img alt="scrapy入门使用-1" src="imgs/16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.assets/scrapy%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8-1-2520822.png" /></p>
<h4 id="scrapy_3">scrapy的核心组件</h4>
<ul>
<li><strong>引擎(Scrapy)</strong>
   用来处理整个系统的数据流处理, 触发事务(框架核心)</li>
<li><strong>调度器(Scheduler)</strong>
   用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
<li><strong>下载器(Downloader)</strong>
   用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
<li><strong>爬虫(Spiders)</strong>
   爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</li>
<li><strong>项目管道(Pipeline)</strong>
   负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
</ul>
<h3 id="4">4、创建爬虫</h3>
<p>命令：<strong>在项目路径下执行</strong>:scrapy genspider +&lt;爬虫名字&gt; + &lt;允许爬取的域名&gt;</p>
<p>示例：</p>
<ul>
<li>scrapy startproject duanzi01</li>
<li>cd duanzi01/</li>
<li>scrapy genspider duanzi duanzixing.com</li>
</ul>
<p>生成的目录和文件结果如下：</p>
<p><img alt="scrapy入门使用-2" src="imgs/16-Scrapy01-%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AE%A4%E8%AF%86.assets/scrapy%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8-2-2520822.png" /></p>
<h3 id="5spider">5、完善spider</h3>
<p>完善spider即通过方法进行数据的提取等操作</p>
<p>在/duanzi01/duanzi01/spiders/duanzi.py中修改内容如下:</p>
<pre><code class="language-python"> import scrapy

 # 自定义spider类，继承scrapy.spider
 class DuanziSpider(scrapy.Spider):
     # 爬虫名字
     name = 'duanzi'
     # 允许爬取的范围，防止爬虫爬到别的网站
     allowed_domains = ['duanzixing.com']
     # 开始爬取的url地址
     start_urls = ['http://duanzixing.com/']

     # 数据提取的方法，接受下载中间件传过来的response 是重写父类中的parse方法
     def parse(self, response, **kwargs):
         # 打印抓取到的页面源码
         # print(response.text)
         # xpath匹配每条段子的article列表
         article_list = response.xpath('//article[@class=&quot;excerpt&quot;]')
         # print(article_list)
         # 循环获取每一个article
         for article in article_list:
             # 匹配标题
             # title = article.xpath('./header/h2/a/text()')
             # [&lt;Selector xpath='./header/h2/a/text()' data='一个不小心就把2000块钱的包包设置成了50包邮'&gt;]
             # title = article.xpath('./header/h2/a/text()')[0].extract()
             # 等同于
             title = article.xpath('./header/h2/a/text()').extract_first()

             # 获取段子内容
             con = article.xpath('./p[@class=&quot;note&quot;]/text()').extract_first()
             print('title', title)
             print('con', con)
</code></pre>
<p><strong>启动爬虫命令</strong>： scrapy crawl  duanzi</p>
<p><strong>response响应对象的常用属性</strong></p>
<ul>
<li>response.url：当前响应的url地址</li>
<li>response.request.url：当前响应对应的请求的url地址</li>
<li>response.headers：响应头</li>
<li>response.request.headers：当前响应的请求头</li>
<li>response.body：响应体，也就是html代码，byte类型</li>
<li>response.status：响应状态码</li>
</ul>
<h5 id="_8">注意：</h5>
<ol>
<li>
<p>response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法</p>
</li>
<li>
<p>extract() 返回一个包含有字符串的列表  </p>
</li>
</ol>
<p><strong>如果使用列表调用extract()则表示，extract会将列表中每一个列表元素进行extract操作，返回列表</strong></p>
<ol>
<li>
<p>extract_first() 返回列表中的第一个字符串，列表为空没有返回None</p>
</li>
<li>
<p>spider中的parse方法必须有</p>
</li>
<li>
<p>需要抓取的url地址必须属于allowed_domains,但是start_urls中的url地址没有这个限制</p>
</li>
<li>
<p>启动爬虫的时候注意启动的位置，是在项目路径下启动</p>
</li>
</ol>
<h3 id="6settings">6、配置settings文件</h3>
<ul>
<li>ROBOTSTXT_OBEY = False</li>
</ul>
<p>robots是一种反爬协议。在协议中规定了哪些身份的爬虫无法爬取的资源有哪些。</p>
<p>在配置文件中setting，取消robots的监测：</p>
<ul>
<li>
<p>在配置文件中配置全局的UA：USER_AGENT='xxxx'</p>
</li>
<li>
<p>在配置文件中加入日志等级：LOG_LEVEL = 'ERROR'  只输出错误信息</p>
</li>
</ul>
<p>其它日志级别</p>
<ul>
<li>CRITICAL  严重错误</li>
<li>ERROR  错误</li>
<li>WARNING  警告</li>
<li>INFO  消息</li>
<li>DEBUG   调试</li>
</ul>
<p>代码实例</p>
<pre><code class="language-python"> # Scrapy settings for mySpider project
 USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'

 ROBOTSTXT_OBEY = False

 LOG_LEVEL = 'ERROR'
</code></pre>
<h3 id="7">7、数据存储</h3>
<h4 id="71">7.1、使用终端命令行进行存储</h4>
<ul>
<li>代码配置</li>
</ul>
<p>/myspider/myspider/spiders/ITSpider.py</p>
<p>```python
   class ITSpider(scrapy.Spider):
       name = 'ITSpider'
       # allowed_domains = ['www.xxx.com']
       start_urls = ['https://duanzixing.com/page/2/']</p>
<pre><code>   # 通过终端写入文件的方式
   def parse(self, response):
       article_list = response.xpath('/html/body/section/div/div/article')
       # 创建列表， 存储数据
       all_data = []
       for article in article_list:
           title = article.xpath('./header/h2/a/text()').extract_first()
           con = article.xpath('./p[2]/text()').extract_first()
           dic = {
               'title': title,
               'con': con
           }
           all_data.append(dic)
       return all_data
</code></pre>
<p>```</p>
<ul>
<li>终端命令</li>
</ul>
<p>scrapy crawl ITSpider -o ITSpider.csv  </p>
<p>将文件存储到ITSpider.csv  文件中</p>
<h4 id="72pipeline">7.2、利用管道pipeline来处理(保存)数据(写入文件中)</h4>
<p><strong>先跟着配置 后面会单讲</strong></p>
<p>代码配置</p>
<ul>
<li>打开items.py文件 添加如下代码</li>
</ul>
<p>myspider/myspider/items.py</p>
<p>```python
   import scrapy</p>
<p>class MyspiderItem(scrapy.Item):
       # define the fields for your item here like:
       # name = scrapy.Field()
       title = scrapy.Field()
       con = scrapy.Field()
  ```</p>
<ul>
<li>/myspider/myspider/spiders/ITSpider.py</li>
</ul>
<p>```python
   import scrapy
   from myspider.items import MyspiderItem</p>
<p>class ITSpiderSpider(scrapy.Spider):
       name = 'ITSpider'
       # allowed_domains = ['www.xxx.com']
       start_urls = ['https://duanzixing.com/page/2/']</p>
<pre><code>   # 写入管道 持久化存储
   def parse(self, response):
       article_list = response.xpath('/html/body/section/div/div/article')
       for article in article_list:
           title = article.xpath('./header/h2/a/text()').extract_first()
           con = article.xpath('./p[2]/text()').extract_first()
           item = DuanziproItem()
           item['title'] = title
           item['con'] = con
           yield item
</code></pre>
<p>```</p>
<p>在爬虫文件ITSpider.py中parse()函数中最后添加</p>
<p><code>python
   yield item</code></p>
<p>思考：为什么要使用yield？</p>
<ol>
<li>让整个函数变成一个生成器，有什么好处呢？</li>
<li>遍历这个函数的返回值的时候，挨个把数据读到内存，不会造成内存的瞬间占用过高</li>
<li>python3中的range和python2中的xrange同理</li>
</ol>
<p><strong>注意：yield能够传递的对象只能是：BaseItem,Request,dict,None</strong></p>
<ul>
<li>打开管道文件 pipelines.py  添加如下代码 </li>
</ul>
<p>myspider/myspider/pipelines.py</p>
<p>```python
   class ITSpiderPipeline:
       f = None
       def open_spider(self, spider):
           print('爬虫开始时被调用一次')
           self.f = open('./duanzi.text', 'w')</p>
<pre><code>   # 爬虫文件中提取数据的方法每yield一次item，就会运行一次
   # 该方法为固定名称函数
   def process_item(self, item, spider):
       print(item)
       self.f.write(item['title']+item['con']+'\n')
       return item

   def close_spider(self, spider):
       print('爬虫结束时被调用')
       self.f.close()
</code></pre>
<p>```</p>
<ul>
<li>
<p>open_spider方法</p>
<p>重写父类中open_spider方法  只有爬虫开始十被调用一次</p>
</li>
<li>
<p>close_spider 方法</p>
<p>重写父类中lose_spider方法  爬虫结束时被调用一次</p>
</li>
<li>
<p>在settings.py设置开启pipeline</p>
</li>
</ul>
<p>将默认被注释的管道打开</p>
<p><code>python
   ITEM_PIPELINES = {
      'myspider.pipelines.MyspiderPipeline': 300,
   }</code></p>
<p>其中数值代表优先级  数值越小优先级越高</p>
<h3 id="8scrapy">8、运行scrapy</h3>
<p>命令：在项目目录下执行scrapy crawl +&lt;爬虫名字&gt;</p>
<p>示例：scrapy crawl ITSpider</p>
<h3 id="9">9、总结</h3>
<ol>
<li>srapy的安装：pip install scrapy</li>
<li>创建scrapy的项目: scrapy startproject myspider</li>
<li>创建scrapy爬虫：在项目目录下执行 scrapy genspider ITSpider <a href="www.xxx.cn">www.xxx.cn</a></li>
<li>运行scrapy爬虫：在项目目录下执行 scrapy crawl ITSpider</li>
<li>解析并获取scrapy爬虫中的数据：</li>
<li>response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法</li>
<li>extract() 返回一个包含有字符串的列表</li>
<li>extract_first() 返回列表中的第一个字符串，列表为空没有返回None</li>
<li>scrapy管道的基本使用:</li>
<li>完善pipelines.py中的process_item函数</li>
<li>在settings.py中设置开启pipeline</li>
</ol>
<h2 id="10">10、报错解决</h2>
<p>如果抓取过程中遇到如下报错，可能是cryptography 版本问题</p>
<pre><code>twisted.web._newclient.ResponseNeverReceived: [&lt;twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', '', 'unsafe legacy renegotiation disabled')]&gt;]
</code></pre>
<p>解决：</p>
<p>1、pip卸载cryptography：</p>
<p>pip uninstall cryptography</p>
<p>重新安装cryptography 36.0.2：</p>
<p>pip install cryptography==36.0.2</p>
<p>2、pip卸载pyOpenSSL：</p>
<p>pip uninstall pyOpenSSL</p>
<p>重新安装pyOpenSSL 22.0.0：</p>
<p>pip install pyOpenSSL==22.0.0</p>
<h5 id="_9"></h5>


</section>
</div> <!-- end of search-noresults -->
<div class="search-results">
<div class="has-results">

<h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
<ul class="search-results-list"></ul>

</div> <!-- end of has-results -->
<div class="no-results">

<h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>

</div> <!-- end of no-results -->
</div> <!-- end of search-results -->
</div> <!-- end of book-search-results -->

</div> <!-- end of page-inner -->
</div> <!-- end of page-wrapper -->

</div> <!-- end of body-inner -->

</div> <!-- end of book-body -->
<script src="./js/main.js"></script>
<script src="search/main.js"></script>
<script src="./js/gitbook.min.js"></script>
<script src="./js/theme.min.js"></script>
</body>
</html>